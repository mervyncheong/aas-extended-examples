{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipping coins with Professor Mittens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline and housekeeping\n",
    "\n",
    "The material in this notebook covers four topics: binomial distributions, the central limit theorem, outliers in data and invalid model assumptions. _You will have approximately 15 minutes to work through each part, after which we will go through the answers together._ Exercises marked as \"extension\" may be more challenging, so you can skip them on a first reading if you feel they will take too much time.\n",
    "\n",
    "This notebook is available on github [here](https://github.com/aezarebski/aas-extended-examples). If you find errors or would like to suggest an improvement, feel free to create an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab we will look at the binomial distribution, central limit theorem, and analyse two data sets collected by [Professor Mittens](https://en.wikipedia.org/wiki/Mittens_(cat)) helping him interrogate the bais in the results of coin flips. Some of the questions are open-ended by design. Partial solutions will be distributed at the end of the session. The imports below are used in the provided solutions, consider these suggestions, not constraints. The answers use `altair` but you can use any plotting library you are comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting altair\n",
      "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
      "\u001b[K     |████████████████████████████████| 727 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from altair) (2.11.2)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /Users/mervyncheong/.local/lib/python3.8/site-packages (from altair) (0.3)\n",
      "Requirement already satisfied: numpy in /Users/mervyncheong/opt/anaconda3/lib/python3.8/site-packages (from altair) (1.19.2)\n",
      "Requirement already satisfied: pandas>=0.18 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from altair) (1.1.3)\n",
      "Requirement already satisfied: jsonschema in /Users/mervyncheong/.local/lib/python3.8/site-packages (from altair) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from jinja2->altair) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from pandas>=0.18->altair) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/mervyncheong/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.18->altair) (2020.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from jsonschema->altair) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from jsonschema->altair) (0.17.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/mervyncheong/.local/lib/python3.8/site-packages (from jsonschema->altair) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/mervyncheong/.local/lib/python3.8/site-packages (from jsonschema->altair) (50.3.0)\n",
      "Installing collected packages: toolz, altair\n",
      "Successfully installed altair-4.1.0 toolz-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import altair as alt\n",
    "from typing import List, Any, Tuple\n",
    "from functools import reduce\n",
    "from itertools import repeat\n",
    "import math as math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation of the binomial distribution\n",
    "\n",
    "Bernoulli and binomial random variables are the typical way to represent the outcome of coin flips. Below we consider estimates of the probability of heads based on a known number of successes in a given number of trials and also a confidence interval (CI) for this based on the Wald method will be given.\n",
    "\n",
    "Let $X$ be a binomial random variable (RV) which results from the number of heads when a coin is flipped $n$ times and the probability of coming up heads is $p$. For the time being we will assume that $n$ is know. The expected value of $X$ is $np$. So a simple way to estimate $p$ is to divide the number of heads, $X$, by the number of flips, $n$. This gives the estimate \n",
    "\n",
    "$$\n",
    "\\hat{p} = X / n.\n",
    "$$\n",
    "\n",
    "It turns out that this is a very sensible thing to do. The resulting estimate is called the maximum likelihood estimate (MLE) of $p$. It is also the result that one obtains via [the method of moments](https://en.wikipedia.org/wiki/Method_of_moments_(statistics)).\n",
    "\n",
    "Given an estimator though, we want to know how confident we are in the estimate it produces. Here we will use the Wald method to get the $95\\%$ CI. It is a very simple method but is acceptable when we have a fair bit of data. The estimated standard error of $\\hat{p}$ is $\\sqrt{\\hat{p}(1-\\hat{p})/n}$, so the Wald CI is given by\n",
    "\n",
    "$$\n",
    "\\hat{p} \\pm z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
    "$$\n",
    "\n",
    "where $z$ is the appropriate quantile of the standard normal distribution. In the case of a $95\\%$ distribution this is just $1.96$.\n",
    "\n",
    "This is stated on the [wikipedia](https://en.wikipedia.org/wiki/Binomial_distribution#Estimation_of_parameters) but there is also a reasonably clear description in [All of Statistics](https://link.springer.com/book/10.1007/978-0-387-21736-9) which you can get via SOLO. You can also find reasonable treatments of Wald CIs in both of those resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 part I (Extension)\n",
    "\n",
    "Professor Mittens is not very familiar with the binomial distribution and wants you to justify the estimator used above. Convince yourself that the estimate given above, $X/n$, is a sensible choice. Prove that it is either the MLE or the method of moments estimator for $p$. State the limitations on the estimator we are using for the CI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "\n",
    "Likelihood function: $\\sum_{i=1}^{n} (x)^p (1-x)^{n-p}$\n",
    "\n",
    "Log-likelihood function: $\\sum_{i=1}^{n} log(x)^p + log(1-x)^{n-p} = \\sum_{i=1}^{n} p log(x) + (n-p) log(1-x)$ \n",
    "\n",
    "Differentiating the log-likelihood function w.r.t $x$: \n",
    "$$\\frac{p}x+ \\frac{n-p}{1-x} = 0$$\n",
    "$$p = \\frac{x}n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 part II\n",
    "\n",
    "Implement a function called `wald_estimate_and_ci` which takes two arguments: `num_trials` which is $n$ in the description above, and `num_success` which is $X$ above. The function should return `(p_hat,(wald_lower,wald_upper))` where `p_hat` is $\\hat{p}$ and `wald_x` are the limits on the $95\\%$ CI using the Wald method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Tuple[float, float]\n",
    "EstimateAndCI = Tuple[float, CI]\n",
    "\n",
    "def wald_estimate_and_ci(num_trials: int, num_success: int):\n",
    "    p_hat = num_success / num_trials\n",
    "    wald_lower = p_hat - 1.96 * np.sqrt((p_hat * (1 - p_hat)) / num_trials)\n",
    "    wald_upper = p_hat + 1.96 * np.sqrt((p_hat * (1 - p_hat)) / num_trials)\n",
    "    return (p_hat, (wald_lower, wald_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 part I\n",
    "\n",
    "Look up how to simulate a random variable from a binomial distribution (it tells you [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html#scipy.stats.binom) if you want to use `scipy`). Then simulate a binomial random variable with $n=100$ and $p=0.6$. Then use the value and the `wald_estimate_and_ci` function to see how well you can estimate $p$. Write a couple of sentences to explain this.\n",
    "\n",
    "### Exercise 2 part II\n",
    "\n",
    "Repeat the process about 100000 times and see what proportion of the CIs capture the true value of $p$. Is it what you expect? Write a couple of sentences to explain what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00092"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, p = 100, 0.6\n",
    "\n",
    "rv = stats.binom.rvs(n, p, size = 100)\n",
    "\n",
    "def ci_contains_value(ci: CI, p: float) -> bool:\n",
    "    lower, upper = ci\n",
    "    return lower < p and p < upper\n",
    "\n",
    "p_in_ci_bools = [ci_contains_value(wald_estimate_and_ci(100, x)[1], 0.6) for x in rv]\n",
    "\n",
    "reduce(lambda a, b: a + 1 if b else a, p_in_ci_bools, 0) / 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 part III\n",
    "\n",
    "Are credible intervals and confidence intervals the same thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central limit theorem\n",
    "\n",
    "The central limit theorem (CLT) tells us about the limiting distribution of the sample mean for distribution for an independent and identically distributed (IID) sample with a finite variance. It underpins many results in statistics and is important for reasoning about stochastic processes.\n",
    "\n",
    "### Exercise 3 part I (Extension)\n",
    "\n",
    "Professor Mittens *really* likes to sound fancy and use the name of important theorems. Write down a statement of the law of large numbers (LLN). Write down a statement of the central limit theorem. Make sure you understand what each of them tells you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "\n",
    "Law of Large Numbers (LLN): $\\bar{X} \\rightarrow \\mu$ as $n \\rightarrow \\infty$. As the sample size gets large, the sample mean tends to the population mean.\n",
    "\n",
    "Central Limit Theorem (CLT): $\\sqrt{n} (\\bar{X} - \\mu) / \\sigma$ As the sample size gets large, the sampling distribution converges towards a standard normal distribution.\n",
    "\n",
    "The LLN tells you what the limiting value is, the CLT tells you about the fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 part II\n",
    "\n",
    "To see that the distribution of the sample mean converges to a normal distribution we will do a simulation study and compare the results with a Q-Q plot to see if it looks normally distributed. This will also demonstrate how to construct a Q-Q plot from first principles, not that you really want to do that. Carry out the following steps:\n",
    "\n",
    "1. Write down the distribution of the sample mean given an IID sample of exponential random variables with rate $1/5$.\n",
    "2. Generate 500 sample means each based on a sample of 100 exponential random variables\n",
    "3. Make a visualisation of the distribution of the data (e.g., a KDE or histogram) and overlay the CLT approximation.\n",
    "4. Make a Q-Q plot to see if the sample means do appear to follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "num_replicates = 500\n",
    "\n",
    "sample_means = [stats.expon.rvs(scale = 5, size = sample_size).mean() for _ in range(num_replicates)]\n",
    "\n",
    "plot_df = pd.DataFrame({\"sample_mean\": sample_means})\n",
    "\n",
    "mesh_size = 200\n",
    "x_vals = [0.02 * ix + 3 for ix in range(0,mesh_size)]\n",
    "\n",
    "clt_scale = 5 / math.sqrt(sample_size)\n",
    "\n",
    "clt_pdf = [stats.norm.pdf(x, loc = 5, scale = clt_scale) for x in x_vals]\n",
    "clt_df = pd.DataFrame({\"x\": x_vals, \"pdf\": clt_pdf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5b1be4e0825b4cd1bb21ff716c8bb6ac\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5b1be4e0825b4cd1bb21ff716c8bb6ac\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5b1be4e0825b4cd1bb21ff716c8bb6ac\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-dc94b661d9f08cf56c6805e4e2592e4c\"}, \"mark\": \"area\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"sample_mean\", \"title\": \"Sample mean\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"density\"}}, \"transform\": [{\"density\": \"sample_mean\", \"extent\": [3, 7], \"as\": [\"sample_mean\", \"density\"]}]}, {\"data\": {\"name\": \"data-7efce130a2d22c3176072cc84f76b386\"}, \"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pdf\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-dc94b661d9f08cf56c6805e4e2592e4c\": [{\"sample_mean\": 4.722713232231899}, {\"sample_mean\": 5.103018047805525}, {\"sample_mean\": 4.57907082873924}, {\"sample_mean\": 5.10124137677968}, {\"sample_mean\": 4.52954931590446}, {\"sample_mean\": 4.4684734624620175}, {\"sample_mean\": 5.144870379750695}, {\"sample_mean\": 5.20169444660745}, {\"sample_mean\": 5.027803850233538}, {\"sample_mean\": 5.255102219559282}, {\"sample_mean\": 4.644905978375058}, {\"sample_mean\": 4.877250391515488}, {\"sample_mean\": 4.782855146309691}, {\"sample_mean\": 4.785601003678921}, {\"sample_mean\": 5.617100001409678}, {\"sample_mean\": 5.193214444905888}, {\"sample_mean\": 5.188207658348765}, {\"sample_mean\": 4.971409282758698}, {\"sample_mean\": 5.092021793145692}, {\"sample_mean\": 5.397544937249908}, {\"sample_mean\": 4.725923673934687}, {\"sample_mean\": 4.815179576361516}, {\"sample_mean\": 4.848020840301431}, {\"sample_mean\": 5.285877339461902}, {\"sample_mean\": 5.342669484795178}, {\"sample_mean\": 5.648262340798547}, {\"sample_mean\": 4.903361983064787}, {\"sample_mean\": 5.0772760914324735}, {\"sample_mean\": 5.287062258017077}, {\"sample_mean\": 5.371643661460351}, {\"sample_mean\": 5.368376775391115}, {\"sample_mean\": 4.849103046752936}, {\"sample_mean\": 4.473737952272561}, {\"sample_mean\": 4.861453739431021}, {\"sample_mean\": 5.129362615311724}, {\"sample_mean\": 5.131217728857029}, {\"sample_mean\": 5.206878351193692}, {\"sample_mean\": 4.982429536378692}, {\"sample_mean\": 4.441553854152324}, {\"sample_mean\": 5.142183361154235}, {\"sample_mean\": 4.904189700838422}, {\"sample_mean\": 5.1503378473254156}, {\"sample_mean\": 5.190916661223648}, {\"sample_mean\": 5.33594756638565}, {\"sample_mean\": 4.628551572322762}, {\"sample_mean\": 5.275742511006965}, {\"sample_mean\": 4.485896653163038}, {\"sample_mean\": 4.041574689188805}, {\"sample_mean\": 4.977534508268116}, {\"sample_mean\": 4.79593085951423}, {\"sample_mean\": 5.236222815677595}, {\"sample_mean\": 4.9997925118131805}, {\"sample_mean\": 5.137624492880713}, {\"sample_mean\": 4.7878658915419825}, {\"sample_mean\": 5.304794974732209}, {\"sample_mean\": 4.805188363455625}, {\"sample_mean\": 4.740809719072484}, {\"sample_mean\": 4.970973993602828}, {\"sample_mean\": 5.222436330013651}, {\"sample_mean\": 4.492994032366025}, {\"sample_mean\": 4.664120172942184}, {\"sample_mean\": 4.744481019072257}, {\"sample_mean\": 4.905504177868884}, {\"sample_mean\": 5.156573521179968}, {\"sample_mean\": 4.4720246068012}, {\"sample_mean\": 5.177006846503148}, {\"sample_mean\": 5.011563127793258}, {\"sample_mean\": 5.018735649956893}, {\"sample_mean\": 4.401297022835058}, {\"sample_mean\": 4.927566382694816}, {\"sample_mean\": 4.934626021969673}, {\"sample_mean\": 4.8578592956433315}, {\"sample_mean\": 5.7082905456451245}, {\"sample_mean\": 4.704243172813653}, {\"sample_mean\": 4.774491914255891}, {\"sample_mean\": 4.526716027836728}, {\"sample_mean\": 5.350853352050424}, {\"sample_mean\": 4.866168639816939}, {\"sample_mean\": 4.979064588975865}, {\"sample_mean\": 4.585071593301824}, {\"sample_mean\": 4.891780227025131}, {\"sample_mean\": 4.869296142804919}, {\"sample_mean\": 4.838632885483119}, {\"sample_mean\": 5.026055654366254}, {\"sample_mean\": 4.9982922748712175}, {\"sample_mean\": 4.847757147502512}, {\"sample_mean\": 5.241105748657842}, {\"sample_mean\": 4.816034380683565}, {\"sample_mean\": 4.7859254005272325}, {\"sample_mean\": 6.3860885893518535}, {\"sample_mean\": 5.599539433816392}, {\"sample_mean\": 5.525104469891029}, {\"sample_mean\": 5.036243697238544}, {\"sample_mean\": 4.617485048066971}, {\"sample_mean\": 4.721743778455305}, {\"sample_mean\": 4.534984934323842}, {\"sample_mean\": 4.982657650660655}, {\"sample_mean\": 5.019233634775677}, {\"sample_mean\": 5.478202204188282}, {\"sample_mean\": 5.200029217631757}, {\"sample_mean\": 5.052260351980116}, {\"sample_mean\": 5.654610515155002}, {\"sample_mean\": 4.189330430776899}, {\"sample_mean\": 5.096890825322571}, {\"sample_mean\": 4.986756339321309}, {\"sample_mean\": 5.663850772634603}, {\"sample_mean\": 5.243543565407252}, {\"sample_mean\": 5.50074581106533}, {\"sample_mean\": 4.642881569700092}, {\"sample_mean\": 5.646308596927911}, {\"sample_mean\": 4.656534356227453}, {\"sample_mean\": 5.06210284593408}, {\"sample_mean\": 4.570557084885813}, {\"sample_mean\": 4.906076478804562}, {\"sample_mean\": 5.762753762142653}, {\"sample_mean\": 4.378935046244569}, {\"sample_mean\": 4.764762968150543}, {\"sample_mean\": 4.933669725891089}, {\"sample_mean\": 4.503146878525936}, {\"sample_mean\": 4.698141457340169}, {\"sample_mean\": 4.6479866416095375}, {\"sample_mean\": 4.474598164749938}, {\"sample_mean\": 5.492127307543335}, {\"sample_mean\": 5.111272176012115}, {\"sample_mean\": 5.329153026675224}, {\"sample_mean\": 5.1140172616032205}, {\"sample_mean\": 5.844410973983822}, {\"sample_mean\": 5.257753609009842}, {\"sample_mean\": 4.9678526974529245}, {\"sample_mean\": 5.1761944144033984}, {\"sample_mean\": 4.564793284513323}, {\"sample_mean\": 4.927246215123798}, {\"sample_mean\": 4.986586262968982}, {\"sample_mean\": 4.642258688614779}, {\"sample_mean\": 5.114514368852573}, {\"sample_mean\": 4.7577363013887375}, {\"sample_mean\": 4.662633343083315}, {\"sample_mean\": 4.554474473467674}, {\"sample_mean\": 4.647051296441235}, {\"sample_mean\": 5.107707330606152}, {\"sample_mean\": 5.134847736957581}, {\"sample_mean\": 4.525811755599916}, {\"sample_mean\": 5.7498006096143}, {\"sample_mean\": 5.357784953210037}, {\"sample_mean\": 5.26999220312312}, {\"sample_mean\": 5.283876505925432}, {\"sample_mean\": 5.5024194070053385}, {\"sample_mean\": 4.7456197953671255}, {\"sample_mean\": 4.831177039337104}, {\"sample_mean\": 4.957453343516716}, {\"sample_mean\": 5.015743427471612}, {\"sample_mean\": 5.008654835574828}, {\"sample_mean\": 5.426587914269785}, {\"sample_mean\": 4.71218068106949}, {\"sample_mean\": 5.059866517099286}, {\"sample_mean\": 4.6674202648864895}, {\"sample_mean\": 5.343608485902256}, {\"sample_mean\": 6.1764989487759525}, {\"sample_mean\": 4.732638476329398}, {\"sample_mean\": 5.230335085780777}, {\"sample_mean\": 5.188757082645543}, {\"sample_mean\": 4.501772457927867}, {\"sample_mean\": 5.110867287899243}, {\"sample_mean\": 4.3702679062897944}, {\"sample_mean\": 5.179357587757811}, {\"sample_mean\": 5.337157344864686}, {\"sample_mean\": 4.998502342538598}, {\"sample_mean\": 5.107361635741113}, {\"sample_mean\": 4.676920400169079}, {\"sample_mean\": 5.111814502767986}, {\"sample_mean\": 4.668714515919749}, {\"sample_mean\": 4.621156788219601}, {\"sample_mean\": 4.449979718585688}, {\"sample_mean\": 5.190774121215875}, {\"sample_mean\": 4.8170102186283215}, {\"sample_mean\": 5.367304811111495}, {\"sample_mean\": 5.0336948307123786}, {\"sample_mean\": 5.077674375817654}, {\"sample_mean\": 5.912292075034993}, {\"sample_mean\": 5.1283713869025975}, {\"sample_mean\": 5.127106463562565}, {\"sample_mean\": 4.859079711377651}, {\"sample_mean\": 4.62585905848253}, {\"sample_mean\": 4.805695984317343}, {\"sample_mean\": 5.110337696028633}, {\"sample_mean\": 4.8669651222880805}, {\"sample_mean\": 4.9223811975557945}, {\"sample_mean\": 4.705455725352679}, {\"sample_mean\": 4.560502571105798}, {\"sample_mean\": 4.987759438481251}, {\"sample_mean\": 4.806192474249683}, {\"sample_mean\": 4.862654371994397}, {\"sample_mean\": 5.919866096135929}, {\"sample_mean\": 5.233128363754807}, {\"sample_mean\": 4.349976511918}, {\"sample_mean\": 4.315928460115596}, {\"sample_mean\": 4.968258102176114}, {\"sample_mean\": 4.88004713874121}, {\"sample_mean\": 4.568062540027288}, {\"sample_mean\": 5.596921569615228}, {\"sample_mean\": 4.845464921269579}, {\"sample_mean\": 5.047295122297279}, {\"sample_mean\": 4.4776489841068345}, {\"sample_mean\": 5.404154684731577}, {\"sample_mean\": 5.614286205153738}, {\"sample_mean\": 5.165032827830157}, {\"sample_mean\": 4.145345965118684}, {\"sample_mean\": 4.915648058263281}, {\"sample_mean\": 4.986705715927238}, {\"sample_mean\": 5.475317301020958}, {\"sample_mean\": 4.534353569440043}, {\"sample_mean\": 5.216864716637222}, {\"sample_mean\": 4.770319404425684}, {\"sample_mean\": 4.949493168899747}, {\"sample_mean\": 5.452613910883091}, {\"sample_mean\": 4.963467590170482}, {\"sample_mean\": 5.52646950882653}, {\"sample_mean\": 5.005256957617553}, {\"sample_mean\": 5.041738487231174}, {\"sample_mean\": 4.522863949351774}, {\"sample_mean\": 4.718527565731013}, {\"sample_mean\": 5.731973304166123}, {\"sample_mean\": 4.499001685747277}, {\"sample_mean\": 5.060305317131627}, {\"sample_mean\": 5.5092621228062395}, {\"sample_mean\": 5.013857635761817}, {\"sample_mean\": 5.073762716343343}, {\"sample_mean\": 4.5757011054207615}, {\"sample_mean\": 5.027614301016538}, {\"sample_mean\": 4.949614015425525}, {\"sample_mean\": 4.408219418724719}, {\"sample_mean\": 4.860581940808486}, {\"sample_mean\": 4.840033111891042}, {\"sample_mean\": 4.959685052133193}, {\"sample_mean\": 5.206414083816078}, {\"sample_mean\": 5.025370175936202}, {\"sample_mean\": 4.990557042982149}, {\"sample_mean\": 4.767689459721106}, {\"sample_mean\": 4.508047049902152}, {\"sample_mean\": 5.2880314313611985}, {\"sample_mean\": 4.7040390771385425}, {\"sample_mean\": 4.516806800777334}, {\"sample_mean\": 4.993654933292682}, {\"sample_mean\": 5.6001238341604225}, {\"sample_mean\": 5.473657581277157}, {\"sample_mean\": 4.919973009459228}, {\"sample_mean\": 5.321369436673149}, {\"sample_mean\": 5.016531833808615}, {\"sample_mean\": 5.123678366795697}, {\"sample_mean\": 4.537683061044938}, {\"sample_mean\": 5.163128329407061}, {\"sample_mean\": 4.383457736279999}, {\"sample_mean\": 4.782520948994178}, {\"sample_mean\": 4.873556890205673}, {\"sample_mean\": 4.892105246865999}, {\"sample_mean\": 5.023755460647315}, {\"sample_mean\": 4.427858380463636}, {\"sample_mean\": 4.905735304098888}, {\"sample_mean\": 5.181023745234411}, {\"sample_mean\": 4.7114800323851105}, {\"sample_mean\": 4.481920764606933}, {\"sample_mean\": 4.798399574589704}, {\"sample_mean\": 4.898879261110909}, {\"sample_mean\": 5.025985657487956}, {\"sample_mean\": 4.601060135242969}, {\"sample_mean\": 4.479859167126397}, {\"sample_mean\": 4.5820520741091615}, {\"sample_mean\": 4.946971110132727}, {\"sample_mean\": 4.9561753790547325}, {\"sample_mean\": 4.769432654363722}, {\"sample_mean\": 4.794759255187583}, {\"sample_mean\": 5.549620198183098}, {\"sample_mean\": 4.843042071243026}, {\"sample_mean\": 4.567751545367269}, {\"sample_mean\": 5.406923624525368}, {\"sample_mean\": 4.959575309485894}, {\"sample_mean\": 4.92355193939095}, {\"sample_mean\": 4.66328844704205}, {\"sample_mean\": 4.407830385357634}, {\"sample_mean\": 5.4738638356110805}, {\"sample_mean\": 5.093006353327244}, {\"sample_mean\": 4.591545876941888}, {\"sample_mean\": 5.183659252640402}, {\"sample_mean\": 4.932008045096759}, {\"sample_mean\": 5.195157337331779}, {\"sample_mean\": 5.045167540598929}, {\"sample_mean\": 5.182739574743718}, {\"sample_mean\": 4.985752478621282}, {\"sample_mean\": 4.860169170967389}, {\"sample_mean\": 4.363452205394053}, {\"sample_mean\": 4.928090378269552}, {\"sample_mean\": 5.085514090755195}, {\"sample_mean\": 5.422678183632788}, {\"sample_mean\": 5.203961702816316}, {\"sample_mean\": 5.348383444307265}, {\"sample_mean\": 5.214704533657584}, {\"sample_mean\": 6.000975590939692}, {\"sample_mean\": 4.6324977795075}, {\"sample_mean\": 5.525098491788846}, {\"sample_mean\": 5.150205643240472}, {\"sample_mean\": 5.0477273665677505}, {\"sample_mean\": 4.7032453475327385}, {\"sample_mean\": 4.954217884907993}, {\"sample_mean\": 5.0446695881981}, {\"sample_mean\": 5.482607536454591}, {\"sample_mean\": 5.234320192394452}, {\"sample_mean\": 5.075674763930585}, {\"sample_mean\": 4.739699617038376}, {\"sample_mean\": 4.771612072303017}, {\"sample_mean\": 4.926085601560267}, {\"sample_mean\": 4.916748080577836}, {\"sample_mean\": 4.6449293179213695}, {\"sample_mean\": 4.880911442587462}, {\"sample_mean\": 5.098100547853806}, {\"sample_mean\": 5.135761740932512}, {\"sample_mean\": 4.62099965664927}, {\"sample_mean\": 4.96208043476878}, {\"sample_mean\": 5.125754481552179}, {\"sample_mean\": 5.4967296809039565}, {\"sample_mean\": 4.812045267250128}, {\"sample_mean\": 5.406546147096911}, {\"sample_mean\": 4.86332208636736}, {\"sample_mean\": 5.160233418095215}, {\"sample_mean\": 5.932643302721357}, {\"sample_mean\": 5.636838030498286}, {\"sample_mean\": 5.440264992304073}, {\"sample_mean\": 4.4398118968250255}, {\"sample_mean\": 5.153877994865584}, {\"sample_mean\": 4.468456082023128}, {\"sample_mean\": 4.61811443104827}, {\"sample_mean\": 4.890797506658011}, {\"sample_mean\": 5.310129068223595}, {\"sample_mean\": 4.6075984382420305}, {\"sample_mean\": 5.463237309996876}, {\"sample_mean\": 5.030846957239588}, {\"sample_mean\": 5.049681488086493}, {\"sample_mean\": 4.993960060889396}, {\"sample_mean\": 4.574394035804553}, {\"sample_mean\": 5.28974089127284}, {\"sample_mean\": 4.753773171519731}, {\"sample_mean\": 5.0410489745573335}, {\"sample_mean\": 5.437827323483633}, {\"sample_mean\": 4.699792914626244}, {\"sample_mean\": 5.869041415039901}, {\"sample_mean\": 5.1490165724233155}, {\"sample_mean\": 4.480644115301783}, {\"sample_mean\": 4.7174502731586285}, {\"sample_mean\": 5.056441808778636}, {\"sample_mean\": 4.095119372840988}, {\"sample_mean\": 5.9703289070234495}, {\"sample_mean\": 5.143462958579476}, {\"sample_mean\": 5.338615661740521}, {\"sample_mean\": 4.970691808765473}, {\"sample_mean\": 5.26410812991159}, {\"sample_mean\": 5.233538176764512}, {\"sample_mean\": 5.466268958831558}, {\"sample_mean\": 4.907940630264697}, {\"sample_mean\": 5.052508748436088}, {\"sample_mean\": 5.059683716895002}, {\"sample_mean\": 5.687325226941516}, {\"sample_mean\": 5.420199153340281}, {\"sample_mean\": 5.1914326638448305}, {\"sample_mean\": 5.018270981580155}, {\"sample_mean\": 5.24724404536965}, {\"sample_mean\": 5.304375876301386}, {\"sample_mean\": 4.978676661561477}, {\"sample_mean\": 5.269324591255987}, {\"sample_mean\": 4.993306355979337}, {\"sample_mean\": 4.889863024833909}, {\"sample_mean\": 5.389831068782667}, {\"sample_mean\": 4.99952131028116}, {\"sample_mean\": 4.946981424145966}, {\"sample_mean\": 5.183880568791438}, {\"sample_mean\": 4.6139364337009905}, {\"sample_mean\": 4.417181888575511}, {\"sample_mean\": 5.251156962155989}, {\"sample_mean\": 4.769141290645933}, {\"sample_mean\": 5.082082420637927}, {\"sample_mean\": 5.501039365593764}, {\"sample_mean\": 5.234695376100941}, {\"sample_mean\": 4.721885230934563}, {\"sample_mean\": 4.871739728768586}, {\"sample_mean\": 5.321689506406893}, {\"sample_mean\": 5.004084662860232}, {\"sample_mean\": 4.843743586414826}, {\"sample_mean\": 5.321230720612464}, {\"sample_mean\": 4.820529783274066}, {\"sample_mean\": 5.003773878079006}, {\"sample_mean\": 4.988772173589796}, {\"sample_mean\": 4.54119053464727}, {\"sample_mean\": 5.479398735968765}, {\"sample_mean\": 5.555677064360494}, {\"sample_mean\": 5.174273627457842}, {\"sample_mean\": 4.884336338885188}, {\"sample_mean\": 4.506616191844121}, {\"sample_mean\": 4.773802410079253}, {\"sample_mean\": 5.294167950069214}, {\"sample_mean\": 4.759714520851464}, {\"sample_mean\": 4.90164517544079}, {\"sample_mean\": 4.971537331987054}, {\"sample_mean\": 5.209155534840054}, {\"sample_mean\": 5.388552859872966}, {\"sample_mean\": 5.110038921849873}, {\"sample_mean\": 5.16333906913139}, {\"sample_mean\": 4.960671526643513}, {\"sample_mean\": 5.035959409393455}, {\"sample_mean\": 4.865005522251291}, {\"sample_mean\": 5.482126741148438}, {\"sample_mean\": 5.138290523829569}, {\"sample_mean\": 5.154945727542729}, {\"sample_mean\": 4.5619484271736495}, {\"sample_mean\": 5.131201485772481}, {\"sample_mean\": 5.133559937331827}, {\"sample_mean\": 4.890837818317358}, {\"sample_mean\": 5.112436406379206}, {\"sample_mean\": 4.820885940253986}, {\"sample_mean\": 5.351664988204009}, {\"sample_mean\": 4.65564948961322}, {\"sample_mean\": 4.766246714714848}, {\"sample_mean\": 5.244058004461275}, {\"sample_mean\": 4.675231289819086}, {\"sample_mean\": 4.558959430996791}, {\"sample_mean\": 4.886491189030125}, {\"sample_mean\": 4.960946585758673}, {\"sample_mean\": 5.18703780347711}, {\"sample_mean\": 5.6722513815297155}, {\"sample_mean\": 4.944583462827159}, {\"sample_mean\": 4.227879554376154}, {\"sample_mean\": 5.142425977242178}, {\"sample_mean\": 4.946709211938043}, {\"sample_mean\": 5.554511336150199}, {\"sample_mean\": 4.71530133872862}, {\"sample_mean\": 4.8926318724144355}, {\"sample_mean\": 5.0199861734684585}, {\"sample_mean\": 4.667205196393308}, {\"sample_mean\": 4.6666431393529475}, {\"sample_mean\": 4.678918240615058}, {\"sample_mean\": 4.7117297763475525}, {\"sample_mean\": 4.307567657622378}, {\"sample_mean\": 5.395415934055975}, {\"sample_mean\": 4.781855689596837}, {\"sample_mean\": 5.079128629450803}, {\"sample_mean\": 4.544655018751554}, {\"sample_mean\": 5.41269447794835}, {\"sample_mean\": 4.81992370252284}, {\"sample_mean\": 5.2397027015070465}, {\"sample_mean\": 4.989773067703028}, {\"sample_mean\": 4.653973275118581}, {\"sample_mean\": 4.5103099162095575}, {\"sample_mean\": 5.327758519241338}, {\"sample_mean\": 4.661488222483345}, {\"sample_mean\": 4.824459476932949}, {\"sample_mean\": 5.56526171441417}, {\"sample_mean\": 5.04290833178414}, {\"sample_mean\": 4.950818794964316}, {\"sample_mean\": 5.605606060449961}, {\"sample_mean\": 5.684510076114676}, {\"sample_mean\": 5.231739235992949}, {\"sample_mean\": 5.153493119958773}, {\"sample_mean\": 5.300835986290361}, {\"sample_mean\": 4.927764459298262}, {\"sample_mean\": 5.551611215927092}, {\"sample_mean\": 5.038033148494072}, {\"sample_mean\": 5.314463593870843}, {\"sample_mean\": 5.26581963599719}, {\"sample_mean\": 4.912499660939644}, {\"sample_mean\": 4.59211472021686}, {\"sample_mean\": 5.390274966345342}, {\"sample_mean\": 5.841508837145807}, {\"sample_mean\": 5.841692875927888}, {\"sample_mean\": 5.013077450587215}, {\"sample_mean\": 4.803878659580139}, {\"sample_mean\": 4.8918618030622625}, {\"sample_mean\": 4.7145198923378135}, {\"sample_mean\": 5.042654219739315}, {\"sample_mean\": 4.629230060271039}, {\"sample_mean\": 4.8091316346045465}, {\"sample_mean\": 4.910940240121473}, {\"sample_mean\": 5.0143097101606715}, {\"sample_mean\": 4.725432528498651}, {\"sample_mean\": 4.760428637823709}, {\"sample_mean\": 4.899057114866697}, {\"sample_mean\": 5.519390057215971}, {\"sample_mean\": 4.964380757092864}, {\"sample_mean\": 4.819779693607369}, {\"sample_mean\": 4.664639151633454}, {\"sample_mean\": 4.9451404367342295}, {\"sample_mean\": 5.364752287345396}, {\"sample_mean\": 4.669001015669675}, {\"sample_mean\": 5.222288658573111}, {\"sample_mean\": 5.221890424132402}, {\"sample_mean\": 4.7567605808469615}, {\"sample_mean\": 4.942319431854479}, {\"sample_mean\": 5.0845063414503215}, {\"sample_mean\": 4.345936017139346}, {\"sample_mean\": 5.4228950310135495}, {\"sample_mean\": 5.897021314476323}, {\"sample_mean\": 4.898279592895692}, {\"sample_mean\": 5.394299431444426}, {\"sample_mean\": 4.645407835565473}], \"data-7efce130a2d22c3176072cc84f76b386\": [{\"x\": 3.0, \"pdf\": 1.2698234671866516e-07}, {\"x\": 3.02, \"pdf\": 1.745913454978549e-07}, {\"x\": 3.04, \"pdf\": 2.392832686493284e-07}, {\"x\": 3.06, \"pdf\": 3.2689796042871425e-07}, {\"x\": 3.08, \"pdf\": 4.4516636871660476e-07}, {\"x\": 3.1, \"pdf\": 6.042862893222472e-07}, {\"x\": 3.12, \"pdf\": 8.176612038467836e-07}, {\"x\": 3.14, \"pdf\": 1.1028445489822057e-06}, {\"x\": 3.16, \"pdf\": 1.4827415837314824e-06}, {\"x\": 3.18, \"pdf\": 1.98713279968088e-06}, {\"x\": 3.2, \"pdf\": 2.6545968447165894e-06}, {\"x\": 3.22, \"pdf\": 3.5349275199902957e-06}, {\"x\": 3.24, \"pdf\": 4.692158896370975e-06}, {\"x\": 3.26, \"pdf\": 6.2083353200127476e-06}, {\"x\": 3.2800000000000002, \"pdf\": 8.188189372620189e-06}, {\"x\": 3.3, \"pdf\": 1.076492103668062e-05}, {\"x\": 3.32, \"pdf\": 1.410730566683488e-05}, {\"x\": 3.34, \"pdf\": 1.8428397100512744e-05}, {\"x\": 3.36, \"pdf\": 2.399613547720731e-05}, {\"x\": 3.38, \"pdf\": 3.1146217073624676e-05}, {\"x\": 3.4, \"pdf\": 4.029763553323548e-05}, {\"x\": 3.42, \"pdf\": 5.197135989491106e-05}, {\"x\": 3.44, \"pdf\": 6.68126741680044e-05}, {\"x\": 3.46, \"pdf\": 8.561776490903225e-05}, {\"x\": 3.48, \"pdf\": 0.00010936520600809304}, {\"x\": 3.5, \"pdf\": 0.00013925305194674762}, {\"x\": 3.52, \"pdf\": 0.000176742309920232}, {\"x\": 3.54, \"pdf\": 0.00022360761467528887}, {\"x\": 3.56, \"pdf\": 0.0002819959743832502}, {\"x\": 3.58, \"pdf\": 0.0003544944874357519}, {\"x\": 3.6, \"pdf\": 0.0004442079442056664}, {\"x\": 3.62, \"pdf\": 0.0005548472194610549}, {\"x\": 3.64, \"pdf\": 0.0006908293246174893}, {\"x\": 3.66, \"pdf\": 0.0008573899182661942}, {\"x\": 3.68, \"pdf\": 0.001060708961985602}, {\"x\": 3.7, \"pdf\": 0.0013080500497232822}, {\"x\": 3.7199999999999998, \"pdf\": 0.0016079137264583395}, {\"x\": 3.74, \"pdf\": 0.0019702048391677224}, {\"x\": 3.76, \"pdf\": 0.002406413624699962}, {\"x\": 3.7800000000000002, \"pdf\": 0.002929809830385715}, {\"x\": 3.8, \"pdf\": 0.003555648680877736}, {\"x\": 3.8200000000000003, \"pdf\": 0.004301386947475875}, {\"x\": 3.84, \"pdf\": 0.00518690674505229}, {\"x\": 3.86, \"pdf\": 0.006234743980477786}, {\"x\": 3.88, \"pdf\": 0.007470317612272713}, {\"x\": 3.9, \"pdf\": 0.008922155064916192}, {\"x\": 3.92, \"pdf\": 0.010622108287775604}, {\"x\": 3.94, \"pdf\": 0.012605554077273774}, {\"x\": 3.96, \"pdf\": 0.014911571415507946}, {\"x\": 3.98, \"pdf\": 0.01758308774738886}, {\"x\": 4.0, \"pdf\": 0.02066698535409204}, {\"x\": 4.02, \"pdf\": 0.02421415831968451}, {\"x\": 4.04, \"pdf\": 0.028279510069901093}, {\"x\": 4.0600000000000005, \"pdf\": 0.03292188112916045}, {\"x\": 4.08, \"pdf\": 0.038203896637112454}, {\"x\": 4.1, \"pdf\": 0.04419172333201094}, {\"x\": 4.12, \"pdf\": 0.050954726185723526}, {\"x\": 4.140000000000001, \"pdf\": 0.058565015701017895}, {\"x\": 4.16, \"pdf\": 0.06709687808675474}, {\"x\": 4.18, \"pdf\": 0.07662608213355307}, {\"x\": 4.2, \"pdf\": 0.08722905863394545}, {\"x\": 4.22, \"pdf\": 0.09898195062734902}, {\"x\": 4.24, \"pdf\": 0.11195953558753903}, {\"x\": 4.26, \"pdf\": 0.12623402387923904}, {\"x\": 4.28, \"pdf\": 0.14187374134473915}, {\"x\": 4.3, \"pdf\": 0.15894170767727767}, {\"x\": 4.32, \"pdf\": 0.17749412621427865}, {\"x\": 4.34, \"pdf\": 0.19757880484240298}, {\"x\": 4.36, \"pdf\": 0.21923353173480786}, {\"x\": 4.38, \"pdf\": 0.24248443351280316}, {\"x\": 4.4, \"pdf\": 0.2673443470035396}, {\"x\": 4.42, \"pdf\": 0.2938112389103993}, {\"x\": 4.4399999999999995, \"pdf\": 0.3218667102803008}, {\"x\": 4.46, \"pdf\": 0.3514746245023804}, {\"x\": 4.48, \"pdf\": 0.38257989857533004}, {\"x\": 4.5, \"pdf\": 0.41510749742059466}, {\"x\": 4.52, \"pdf\": 0.4489616700010925}, {\"x\": 4.54, \"pdf\": 0.4840254638617831}, {\"x\": 4.5600000000000005, \"pdf\": 0.5201605514020323}, {\"x\": 4.58, \"pdf\": 0.5572073967184715}, {\"x\": 4.6, \"pdf\": 0.5949857862574683}, {\"x\": 4.62, \"pdf\": 0.633295739865391}, {\"x\": 4.640000000000001, \"pdf\": 0.6719188112401959}, {\"x\": 4.66, \"pdf\": 0.7106197784266876}, {\"x\": 4.68, \"pdf\": 0.7491487160540626}, {\"x\": 4.7, \"pdf\": 0.7872434317142878}, {\"x\": 4.72, \"pdf\": 0.8246322394759612}, {\"x\": 4.74, \"pdf\": 0.8610370342917477}, {\"x\": 4.76, \"pdf\": 0.8961766222695111}, {\"x\": 4.78, \"pdf\": 0.929770253719997}, {\"x\": 4.8, \"pdf\": 0.9615412988393077}, {\"x\": 4.82, \"pdf\": 0.9912210000854449}, {\"x\": 4.84, \"pdf\": 1.0185522309873223}, {\"x\": 4.86, \"pdf\": 1.0432931884662069}, {\"x\": 4.88, \"pdf\": 1.0652209448901007}, {\"x\": 4.9, \"pdf\": 1.0841347871048637}, {\"x\": 4.92, \"pdf\": 1.0998592726166878}, {\"x\": 4.9399999999999995, \"pdf\": 1.112246937903477}, {\"x\": 4.96, \"pdf\": 1.1211806004105842}, {\"x\": 4.98, \"pdf\": 1.126575203983495}, {\"x\": 5.0, \"pdf\": 1.1283791670955128}, {\"x\": 5.02, \"pdf\": 1.126575203983495}, {\"x\": 5.04, \"pdf\": 1.1211806004105842}, {\"x\": 5.0600000000000005, \"pdf\": 1.112246937903477}, {\"x\": 5.08, \"pdf\": 1.0998592726166878}, {\"x\": 5.1, \"pdf\": 1.0841347871048637}, {\"x\": 5.12, \"pdf\": 1.0652209448901007}, {\"x\": 5.140000000000001, \"pdf\": 1.0432931884662056}, {\"x\": 5.16, \"pdf\": 1.0185522309873223}, {\"x\": 5.18, \"pdf\": 0.9912210000854449}, {\"x\": 5.2, \"pdf\": 0.9615412988393077}, {\"x\": 5.220000000000001, \"pdf\": 0.9297702537199956}, {\"x\": 5.24, \"pdf\": 0.8961766222695111}, {\"x\": 5.26, \"pdf\": 0.8610370342917477}, {\"x\": 5.28, \"pdf\": 0.8246322394759612}, {\"x\": 5.300000000000001, \"pdf\": 0.787243431714286}, {\"x\": 5.32, \"pdf\": 0.7491487160540626}, {\"x\": 5.34, \"pdf\": 0.7106197784266876}, {\"x\": 5.359999999999999, \"pdf\": 0.6719188112401959}, {\"x\": 5.38, \"pdf\": 0.633295739865391}, {\"x\": 5.4, \"pdf\": 0.5949857862574683}, {\"x\": 5.42, \"pdf\": 0.5572073967184715}, {\"x\": 5.4399999999999995, \"pdf\": 0.5201605514020323}, {\"x\": 5.46, \"pdf\": 0.4840254638617831}, {\"x\": 5.48, \"pdf\": 0.4489616700010925}, {\"x\": 5.5, \"pdf\": 0.41510749742059466}, {\"x\": 5.52, \"pdf\": 0.38257989857533004}, {\"x\": 5.54, \"pdf\": 0.3514746245023804}, {\"x\": 5.5600000000000005, \"pdf\": 0.3218667102803008}, {\"x\": 5.58, \"pdf\": 0.2938112389103993}, {\"x\": 5.6, \"pdf\": 0.2673443470035396}, {\"x\": 5.62, \"pdf\": 0.24248443351280316}, {\"x\": 5.640000000000001, \"pdf\": 0.2192335317348069}, {\"x\": 5.66, \"pdf\": 0.19757880484240298}, {\"x\": 5.68, \"pdf\": 0.17749412621427865}, {\"x\": 5.7, \"pdf\": 0.15894170767727767}, {\"x\": 5.720000000000001, \"pdf\": 0.14187374134473843}, {\"x\": 5.74, \"pdf\": 0.12623402387923904}, {\"x\": 5.76, \"pdf\": 0.11195953558753903}, {\"x\": 5.78, \"pdf\": 0.09898195062734902}, {\"x\": 5.800000000000001, \"pdf\": 0.08722905863394495}, {\"x\": 5.82, \"pdf\": 0.07662608213355307}, {\"x\": 5.84, \"pdf\": 0.06709687808675474}, {\"x\": 5.859999999999999, \"pdf\": 0.058565015701017895}, {\"x\": 5.88, \"pdf\": 0.050954726185723526}, {\"x\": 5.9, \"pdf\": 0.04419172333201094}, {\"x\": 5.92, \"pdf\": 0.038203896637112454}, {\"x\": 5.9399999999999995, \"pdf\": 0.03292188112916045}, {\"x\": 5.96, \"pdf\": 0.028279510069901093}, {\"x\": 5.98, \"pdf\": 0.02421415831968451}, {\"x\": 6.0, \"pdf\": 0.02066698535409204}, {\"x\": 6.02, \"pdf\": 0.01758308774738892}, {\"x\": 6.04, \"pdf\": 0.014911571415507946}, {\"x\": 6.0600000000000005, \"pdf\": 0.012605554077273729}, {\"x\": 6.08, \"pdf\": 0.010622108287775604}, {\"x\": 6.1, \"pdf\": 0.008922155064916231}, {\"x\": 6.12, \"pdf\": 0.007470317612272713}, {\"x\": 6.140000000000001, \"pdf\": 0.006234743980477765}, {\"x\": 6.16, \"pdf\": 0.00518690674505229}, {\"x\": 6.18, \"pdf\": 0.004301386947475875}, {\"x\": 6.2, \"pdf\": 0.003555648680877736}, {\"x\": 6.220000000000001, \"pdf\": 0.002929809830385691}, {\"x\": 6.24, \"pdf\": 0.002406413624699962}, {\"x\": 6.26, \"pdf\": 0.0019702048391677224}, {\"x\": 6.28, \"pdf\": 0.0016079137264583395}, {\"x\": 6.300000000000001, \"pdf\": 0.0013080500497232694}, {\"x\": 6.32, \"pdf\": 0.001060708961985599}, {\"x\": 6.34, \"pdf\": 0.0008573899182661942}, {\"x\": 6.359999999999999, \"pdf\": 0.0006908293246174929}, {\"x\": 6.38, \"pdf\": 0.0005548472194610549}, {\"x\": 6.4, \"pdf\": 0.00044420794420566397}, {\"x\": 6.42, \"pdf\": 0.0003544944874357519}, {\"x\": 6.4399999999999995, \"pdf\": 0.0002819959743832512}, {\"x\": 6.46, \"pdf\": 0.00022360761467528887}, {\"x\": 6.48, \"pdf\": 0.0001767423099202314}, {\"x\": 6.5, \"pdf\": 0.00013925305194674762}, {\"x\": 6.52, \"pdf\": 0.00010936520600809382}, {\"x\": 6.54, \"pdf\": 8.561776490903225e-05}, {\"x\": 6.5600000000000005, \"pdf\": 6.681267416800381e-05}, {\"x\": 6.58, \"pdf\": 5.197135989491106e-05}, {\"x\": 6.6, \"pdf\": 4.0297635533235756e-05}, {\"x\": 6.62, \"pdf\": 3.1146217073624676e-05}, {\"x\": 6.640000000000001, \"pdf\": 2.39961354772071e-05}, {\"x\": 6.66, \"pdf\": 1.8428397100512744e-05}, {\"x\": 6.68, \"pdf\": 1.4107305666834931e-05}, {\"x\": 6.7, \"pdf\": 1.076492103668062e-05}, {\"x\": 6.720000000000001, \"pdf\": 8.188189372620087e-06}, {\"x\": 6.74, \"pdf\": 6.2083353200127476e-06}, {\"x\": 6.76, \"pdf\": 4.692158896370975e-06}, {\"x\": 6.78, \"pdf\": 3.5349275199902647e-06}, {\"x\": 6.800000000000001, \"pdf\": 2.6545968447165564e-06}, {\"x\": 6.82, \"pdf\": 1.9871327996808693e-06}, {\"x\": 6.84, \"pdf\": 1.4827415837314824e-06}, {\"x\": 6.859999999999999, \"pdf\": 1.1028445489822155e-06}, {\"x\": 6.88, \"pdf\": 8.176612038467836e-07}, {\"x\": 6.9, \"pdf\": 6.042862893222408e-07}, {\"x\": 6.92, \"pdf\": 4.4516636871660476e-07}, {\"x\": 6.9399999999999995, \"pdf\": 3.268979604287172e-07}, {\"x\": 6.96, \"pdf\": 2.392832686493284e-07}, {\"x\": 6.98, \"pdf\": 1.7459134549785337e-07}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kde = (alt\n",
    "                .Chart(plot_df)\n",
    "                .transform_density(\n",
    "                    'sample_mean',\n",
    "                    as_=['sample_mean', 'density'],\n",
    "                    extent=[3,7]\n",
    "                )\n",
    "                .mark_area()\n",
    "                .encode(x = alt.X(\"sample_mean:Q\",\n",
    "                                 title=\"Sample mean\"),\n",
    "                        y = \"density:Q\"))\n",
    "\n",
    "clt_line = (alt\n",
    "               .Chart(clt_df)\n",
    "               .mark_line(color = \"red\")\n",
    "               .encode(x = \"x\",\n",
    "                       y = \"pdf\"))\n",
    "\n",
    "data_kde + clt_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-dea7f98baae14e2d95f3471ed29ce3fd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-dea7f98baae14e2d95f3471ed29ce3fd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-dea7f98baae14e2d95f3471ed29ce3fd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-4b33247a47d51bf30c6d05d360fe2183\"}, \"mark\": \"point\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"sample_means\", \"scale\": {\"zero\": false}, \"title\": \"Sample mean quantile\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"quantiles\", \"scale\": {\"zero\": false}, \"title\": \"Normal quantile\"}}}, {\"data\": {\"name\": \"data-48b6d2e21e0069851a24f831171f2869\"}, \"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-4b33247a47d51bf30c6d05d360fe2183\": [{\"sample_means\": 4.041574689188805, \"quantiles\": 3.9074378904334983}, {\"sample_means\": 4.095119372840988, \"quantiles\": 4.0285125745668395}, {\"sample_means\": 4.145345965118684, \"quantiles\": 4.089306816140775}, {\"sample_means\": 4.189330430776899, \"quantiles\": 4.131226196812145}, {\"sample_means\": 4.227879554376154, \"quantiles\": 4.16362769039822}, {\"sample_means\": 4.307567657622378, \"quantiles\": 4.190232671078349}, {\"sample_means\": 4.315928460115596, \"quantiles\": 4.212915280779262}, {\"sample_means\": 4.345936017139346, \"quantiles\": 4.232757189111141}, {\"sample_means\": 4.349976511918, \"quantiles\": 4.250441465790852}, {\"sample_means\": 4.363452205394053, \"quantiles\": 4.266428073666739}, {\"sample_means\": 4.3702679062897944, \"quantiles\": 4.281042056391843}, {\"sample_means\": 4.378935046244569, \"quantiles\": 4.29452192962303}, {\"sample_means\": 4.383457736279999, \"quantiles\": 4.307048087825161}, {\"sample_means\": 4.401297022835058, \"quantiles\": 4.31876039640342}, {\"sample_means\": 4.407830385357634, \"quantiles\": 4.329769571432056}, {\"sample_means\": 4.408219418724719, \"quantiles\": 4.340164812050592}, {\"sample_means\": 4.417181888575511, \"quantiles\": 4.350019078390521}, {\"sample_means\": 4.427858380463636, \"quantiles\": 4.359392838125469}, {\"sample_means\": 4.4398118968250255, \"quantiles\": 4.3683367869505245}, {\"sample_means\": 4.441553854152324, \"quantiles\": 4.376893863574251}, {\"sample_means\": 4.449979718585688, \"quantiles\": 4.3851007685264936}, {\"sample_means\": 4.468456082023128, \"quantiles\": 4.39298912692152}, {\"sample_means\": 4.4684734624620175, \"quantiles\": 4.400586391129214}, {\"sample_means\": 4.4720246068012, \"quantiles\": 4.407916550377942}, {\"sample_means\": 4.473737952272561, \"quantiles\": 4.41500069495354}, {\"sample_means\": 4.474598164749938, \"quantiles\": 4.421857469445889}, {\"sample_means\": 4.4776489841068345, \"quantiles\": 4.428503440313997}, {\"sample_means\": 4.479859167126397, \"quantiles\": 4.434953396557377}, {\"sample_means\": 4.480644115301783, \"quantiles\": 4.441220597634742}, {\"sample_means\": 4.481920764606933, \"quantiles\": 4.447316979394846}, {\"sample_means\": 4.485896653163038, \"quantiles\": 4.453253326300384}, {\"sample_means\": 4.492994032366025, \"quantiles\": 4.459039416376998}, {\"sample_means\": 4.499001685747277, \"quantiles\": 4.464684143928526}, {\"sample_means\": 4.501772457927867, \"quantiles\": 4.470195624002706}, {\"sample_means\": 4.503146878525936, \"quantiles\": 4.475581281780869}, {\"sample_means\": 4.506616191844121, \"quantiles\": 4.480847929438017}, {\"sample_means\": 4.508047049902152, \"quantiles\": 4.486001832530498}, {\"sample_means\": 4.5103099162095575, \"quantiles\": 4.491048767583986}, {\"sample_means\": 4.516806800777334, \"quantiles\": 4.495994072250183}, {\"sample_means\": 4.522863949351774, \"quantiles\": 4.500842689158024}, {\"sample_means\": 4.525811755599916, \"quantiles\": 4.5055992043906805}, {\"sample_means\": 4.526716027836728, \"quantiles\": 4.510267881362602}, {\"sample_means\": 4.52954931590446, \"quantiles\": 4.514852690743496}, {\"sample_means\": 4.534353569440043, \"quantiles\": 4.519357336972134}, {\"sample_means\": 4.534984934323842, \"quantiles\": 4.5237852818176645}, {\"sample_means\": 4.537683061044938, \"quantiles\": 4.528139765375799}, {\"sample_means\": 4.54119053464727, \"quantiles\": 4.532423824829105}, {\"sample_means\": 4.544655018751554, \"quantiles\": 4.536640311252236}, {\"sample_means\": 4.554474473467674, \"quantiles\": 4.540791904702492}, {\"sample_means\": 4.558959430996791, \"quantiles\": 4.544881127802275}, {\"sample_means\": 4.560502571105798, \"quantiles\": 4.548910357991415}, {\"sample_means\": 4.5619484271736495, \"quantiles\": 4.552881838603297}, {\"sample_means\": 4.564793284513323, \"quantiles\": 4.556797688898229}, {\"sample_means\": 4.567751545367269, \"quantiles\": 4.560659913170154}, {\"sample_means\": 4.568062540027288, \"quantiles\": 4.564470409027941}, {\"sample_means\": 4.570557084885813, \"quantiles\": 4.568230974939771}, {\"sample_means\": 4.574394035804553, \"quantiles\": 4.571943317118258}, {\"sample_means\": 4.5757011054207615, \"quantiles\": 4.575609055814519}, {\"sample_means\": 4.57907082873924, \"quantiles\": 4.579229731081292}, {\"sample_means\": 4.5820520741091615, \"quantiles\": 4.582806808058165}, {\"sample_means\": 4.585071593301824, \"quantiles\": 4.586341681825912}, {\"sample_means\": 4.591545876941888, \"quantiles\": 4.589835681871555}, {\"sample_means\": 4.59211472021686, \"quantiles\": 4.593290076201191}, {\"sample_means\": 4.601060135242969, \"quantiles\": 4.596706075133549}, {\"sample_means\": 4.6075984382420305, \"quantiles\": 4.600084834803677}, {\"sample_means\": 4.6139364337009905, \"quantiles\": 4.6034274604030525}, {\"sample_means\": 4.617485048066971, \"quantiles\": 4.60673500917966}, {\"sample_means\": 4.61811443104827, \"quantiles\": 4.610008493219149}, {\"sample_means\": 4.62099965664927, \"quantiles\": 4.613248882026049}, {\"sample_means\": 4.621156788219601, \"quantiles\": 4.61645710492212}, {\"sample_means\": 4.62585905848253, \"quantiles\": 4.6196340532772355}, {\"sample_means\": 4.628551572322762, \"quantiles\": 4.622780582586689}, {\"sample_means\": 4.629230060271039, \"quantiles\": 4.625897514407508}, {\"sample_means\": 4.6324977795075, \"quantiles\": 4.628985638165144}, {\"sample_means\": 4.642258688614779, \"quantiles\": 4.632045712840866}, {\"sample_means\": 4.642881569700092, \"quantiles\": 4.63507846854922}, {\"sample_means\": 4.644905978375058, \"quantiles\": 4.638084608014093}, {\"sample_means\": 4.6449293179213695, \"quantiles\": 4.641064807951123}, {\"sample_means\": 4.645407835565473, \"quantiles\": 4.644019720363531}, {\"sample_means\": 4.647051296441235, \"quantiles\": 4.646949973757847}, {\"sample_means\": 4.6479866416095375, \"quantiles\": 4.649856174285401}, {\"sample_means\": 4.653973275118581, \"quantiles\": 4.652738906814999}, {\"sample_means\": 4.65564948961322, \"quantiles\": 4.655598735941722}, {\"sample_means\": 4.656534356227453, \"quantiles\": 4.658436206936368}, {\"sample_means\": 4.661488222483345, \"quantiles\": 4.661251846639718}, {\"sample_means\": 4.662633343083315, \"quantiles\": 4.664046164305441}, {\"sample_means\": 4.66328844704205, \"quantiles\": 4.66681965239514}, {\"sample_means\": 4.664120172942184, \"quantiles\": 4.669572787328807}, {\"sample_means\": 4.664639151633454, \"quantiles\": 4.672306030193648}, {\"sample_means\": 4.6666431393529475, \"quantiles\": 4.675019827414043}, {\"sample_means\": 4.667205196393308, \"quantiles\": 4.6777146113851735}, {\"sample_means\": 4.6674202648864895, \"quantiles\": 4.680390801072691}, {\"sample_means\": 4.668714515919749, \"quantiles\": 4.683048802580564}, {\"sample_means\": 4.669001015669675, \"quantiles\": 4.685689009689157}, {\"sample_means\": 4.675231289819086, \"quantiles\": 4.688311804365373}, {\"sample_means\": 4.676920400169079, \"quantiles\": 4.690917557246616}, {\"sample_means\": 4.678918240615058, \"quantiles\": 4.693506628100157}, {\"sample_means\": 4.698141457340169, \"quantiles\": 4.6960793662594185}, {\"sample_means\": 4.699792914626244, \"quantiles\": 4.698636111038546}, {\"sample_means\": 4.7032453475327385, \"quantiles\": 4.701177192126562}, {\"sample_means\": 4.7040390771385425, \"quantiles\": 4.703702929962316}, {\"sample_means\": 4.704243172813653, \"quantiles\": 4.70621363609133}, {\"sample_means\": 4.705455725352679, \"quantiles\": 4.708709613505602}, {\"sample_means\": 4.7114800323851105, \"quantiles\": 4.711191156967332}, {\"sample_means\": 4.7117297763475525, \"quantiles\": 4.713658553317482}, {\"sample_means\": 4.71218068106949, \"quantiles\": 4.7161120817700235}, {\"sample_means\": 4.7145198923378135, \"quantiles\": 4.718552014192659}, {\"sample_means\": 4.71530133872862, \"quantiles\": 4.720978615374777}, {\"sample_means\": 4.7174502731586285, \"quantiles\": 4.723392143283312}, {\"sample_means\": 4.718527565731013, \"quantiles\": 4.725792849307199}, {\"sample_means\": 4.721743778455305, \"quantiles\": 4.728180978490986}, {\"sample_means\": 4.721885230934563, \"quantiles\": 4.730556769758226}, {\"sample_means\": 4.722713232231899, \"quantiles\": 4.732920456125149}, {\"sample_means\": 4.725432528498651, \"quantiles\": 4.735272264905146}, {\"sample_means\": 4.725923673934687, \"quantiles\": 4.737612417904525}, {\"sample_means\": 4.732638476329398, \"quantiles\": 4.739941131609988}, {\"sample_means\": 4.739699617038376, \"quantiles\": 4.742258617368254}, {\"sample_means\": 4.740809719072484, \"quantiles\": 4.744565081558219}, {\"sample_means\": 4.744481019072257, \"quantiles\": 4.746860725756031}, {\"sample_means\": 4.7456197953671255, \"quantiles\": 4.749145746893416}, {\"sample_means\": 4.753773171519731, \"quantiles\": 4.751420337409606}, {\"sample_means\": 4.7567605808469615, \"quantiles\": 4.753684685397165}, {\"sample_means\": 4.7577363013887375, \"quantiles\": 4.755938974742021}, {\"sample_means\": 4.759714520851464, \"quantiles\": 4.758183385257961}, {\"sample_means\": 4.760428637823709, \"quantiles\": 4.760418092815877}, {\"sample_means\": 4.764762968150543, \"quantiles\": 4.762643269467992}, {\"sample_means\": 4.766246714714848, \"quantiles\": 4.764859083567301}, {\"sample_means\": 4.767689459721106, \"quantiles\": 4.767065699882472}, {\"sample_means\": 4.769141290645933, \"quantiles\": 4.769263279708377}, {\"sample_means\": 4.769432654363722, \"quantiles\": 4.771451980972493}, {\"sample_means\": 4.770319404425684, \"quantiles\": 4.7736319583373295}, {\"sample_means\": 4.771612072303017, \"quantiles\": 4.775803363299072}, {\"sample_means\": 4.773802410079253, \"quantiles\": 4.777966344282629}, {\"sample_means\": 4.774491914255891, \"quantiles\": 4.780121046733197}, {\"sample_means\": 4.781855689596837, \"quantiles\": 4.782267613204553}, {\"sample_means\": 4.782520948994178, \"quantiles\": 4.7844061834441645}, {\"sample_means\": 4.782855146309691, \"quantiles\": 4.786536894475296}, {\"sample_means\": 4.785601003678921, \"quantiles\": 4.788659880676219}, {\"sample_means\": 4.7859254005272325, \"quantiles\": 4.790775273856649}, {\"sample_means\": 4.7878658915419825, \"quantiles\": 4.792883203331545}, {\"sample_means\": 4.794759255187583, \"quantiles\": 4.794983795992361}, {\"sample_means\": 4.79593085951423, \"quantiles\": 4.797077176375871}, {\"sample_means\": 4.798399574589704, \"quantiles\": 4.799163466730663}, {\"sample_means\": 4.803878659580139, \"quantiles\": 4.8012427870813985}, {\"sample_means\": 4.805188363455625, \"quantiles\": 4.803315255290928}, {\"sample_means\": 4.805695984317343, \"quantiles\": 4.8053809871203566}, {\"sample_means\": 4.806192474249683, \"quantiles\": 4.807440096287129}, {\"sample_means\": 4.8091316346045465, \"quantiles\": 4.809492694521234}, {\"sample_means\": 4.812045267250128, \"quantiles\": 4.8115388916195805}, {\"sample_means\": 4.815179576361516, \"quantiles\": 4.813578795498638}, {\"sample_means\": 4.816034380683565, \"quantiles\": 4.815612512245394}, {\"sample_means\": 4.8170102186283215, \"quantiles\": 4.81764014616671}, {\"sample_means\": 4.819779693607369, \"quantiles\": 4.819661799837121}, {\"sample_means\": 4.81992370252284, \"quantiles\": 4.8216775741451565}, {\"sample_means\": 4.820529783274066, \"quantiles\": 4.823687568338226}, {\"sample_means\": 4.820885940253986, \"quantiles\": 4.825691880066132}, {\"sample_means\": 4.824459476932949, \"quantiles\": 4.827690605423262}, {\"sample_means\": 4.831177039337104, \"quantiles\": 4.829683838989503}, {\"sample_means\": 4.838632885483119, \"quantiles\": 4.831671673869937}, {\"sample_means\": 4.840033111891042, \"quantiles\": 4.833654201733354}, {\"sample_means\": 4.843042071243026, \"quantiles\": 4.835631512849632}, {\"sample_means\": 4.843743586414826, \"quantiles\": 4.837603696126029}, {\"sample_means\": 4.845464921269579, \"quantiles\": 4.839570839142409}, {\"sample_means\": 4.847757147502512, \"quantiles\": 4.841533028185469}, {\"sample_means\": 4.848020840301431, \"quantiles\": 4.843490348281982}, {\"sample_means\": 4.849103046752936, \"quantiles\": 4.845442883231099}, {\"sample_means\": 4.8578592956433315, \"quantiles\": 4.847390715635748}, {\"sample_means\": 4.859079711377651, \"quantiles\": 4.849333926933147}, {\"sample_means\": 4.860169170967389, \"quantiles\": 4.851272597424491}, {\"sample_means\": 4.860581940808486, \"quantiles\": 4.85320680630381}, {\"sample_means\": 4.861453739431021, \"quantiles\": 4.855136631686047}, {\"sample_means\": 4.862654371994397, \"quantiles\": 4.857062150634389}, {\"sample_means\": 4.86332208636736, \"quantiles\": 4.858983439186849}, {\"sample_means\": 4.865005522251291, \"quantiles\": 4.86090057238216}, {\"sample_means\": 4.866168639816939, \"quantiles\": 4.862813624284983}, {\"sample_means\": 4.8669651222880805, \"quantiles\": 4.864722668010452}, {\"sample_means\": 4.869296142804919, \"quantiles\": 4.866627775748098}, {\"sample_means\": 4.871739728768586, \"quantiles\": 4.868529018785152}, {\"sample_means\": 4.873556890205673, \"quantiles\": 4.870426467529256}, {\"sample_means\": 4.877250391515488, \"quantiles\": 4.872320191530613}, {\"sample_means\": 4.88004713874121, \"quantiles\": 4.874210259503574}, {\"sample_means\": 4.880911442587462, \"quantiles\": 4.876096739347701}, {\"sample_means\": 4.884336338885188, \"quantiles\": 4.877979698168309}, {\"sample_means\": 4.886491189030125, \"quantiles\": 4.879859202296521}, {\"sample_means\": 4.889863024833909, \"quantiles\": 4.881735317308828}, {\"sample_means\": 4.890797506658011, \"quantiles\": 4.883608108046202}, {\"sample_means\": 4.890837818317358, \"quantiles\": 4.885477638632744}, {\"sample_means\": 4.891780227025131, \"quantiles\": 4.887343972493911}, {\"sample_means\": 4.8918618030622625, \"quantiles\": 4.889207172374319}, {\"sample_means\": 4.892105246865999, \"quantiles\": 4.891067300355139}, {\"sample_means\": 4.8926318724144355, \"quantiles\": 4.892924417871107}, {\"sample_means\": 4.898279592895692, \"quantiles\": 4.894778585727152}, {\"sample_means\": 4.898879261110909, \"quantiles\": 4.896629864114657}, {\"sample_means\": 4.899057114866697, \"quantiles\": 4.898478312627372}, {\"sample_means\": 4.90164517544079, \"quantiles\": 4.900323990276978}, {\"sample_means\": 4.903361983064787, \"quantiles\": 4.902166955508328}, {\"sample_means\": 4.904189700838422, \"quantiles\": 4.904007266214362}, {\"sample_means\": 4.905504177868884, \"quantiles\": 4.905844979750718}, {\"sample_means\": 4.905735304098888, \"quantiles\": 4.907680152950043}, {\"sample_means\": 4.906076478804562, \"quantiles\": 4.909512842136012}, {\"sample_means\": 4.907940630264697, \"quantiles\": 4.911343103137077}, {\"sample_means\": 4.910940240121473, \"quantiles\": 4.913170991299937}, {\"sample_means\": 4.912499660939644, \"quantiles\": 4.914996561502753}, {\"sample_means\": 4.915648058263281, \"quantiles\": 4.916819868168111}, {\"sample_means\": 4.916748080577836, \"quantiles\": 4.918640965275739}, {\"sample_means\": 4.919973009459228, \"quantiles\": 4.9204599063749965}, {\"sample_means\": 4.9223811975557945, \"quantiles\": 4.9222767445971245}, {\"sample_means\": 4.92355193939095, \"quantiles\": 4.9240915326672905}, {\"sample_means\": 4.926085601560267, \"quantiles\": 4.925904322916414}, {\"sample_means\": 4.927246215123798, \"quantiles\": 4.927715167292788}, {\"sample_means\": 4.927566382694816, \"quantiles\": 4.929524117373505}, {\"sample_means\": 4.927764459298262, \"quantiles\": 4.931331224375699}, {\"sample_means\": 4.928090378269552, \"quantiles\": 4.93313653916759}, {\"sample_means\": 4.932008045096759, \"quantiles\": 4.934940112279367}, {\"sample_means\": 4.933669725891089, \"quantiles\": 4.93674199391389}, {\"sample_means\": 4.934626021969673, \"quantiles\": 4.938542233957235}, {\"sample_means\": 4.942319431854479, \"quantiles\": 4.940340881989074}, {\"sample_means\": 4.944583462827159, \"quantiles\": 4.94213798729291}, {\"sample_means\": 4.9451404367342295, \"quantiles\": 4.943933598866163}, {\"sample_means\": 4.946709211938043, \"quantiles\": 4.945727765430113}, {\"sample_means\": 4.946971110132727, \"quantiles\": 4.947520535439707}, {\"sample_means\": 4.946981424145966, \"quantiles\": 4.9493119570932445}, {\"sample_means\": 4.949493168899747, \"quantiles\": 4.951102078341928}, {\"sample_means\": 4.949614015425525, \"quantiles\": 4.952890946899301}, {\"sample_means\": 4.950818794964316, \"quantiles\": 4.954678610250569}, {\"sample_means\": 4.954217884907993, \"quantiles\": 4.956465115661806}, {\"sample_means\": 4.9561753790547325, \"quantiles\": 4.958250510189068}, {\"sample_means\": 4.957453343516716, \"quantiles\": 4.960034840687398}, {\"sample_means\": 4.959575309485894, \"quantiles\": 4.961818153819733}, {\"sample_means\": 4.959685052133193, \"quantiles\": 4.963600496065727}, {\"sample_means\": 4.960671526643513, \"quantiles\": 4.965381913730485}, {\"sample_means\": 4.960946585758673, \"quantiles\": 4.967162452953215}, {\"sample_means\": 4.96208043476878, \"quantiles\": 4.968942159715798}, {\"sample_means\": 4.963467590170482, \"quantiles\": 4.970721079851293}, {\"sample_means\": 4.964380757092864, \"quantiles\": 4.972499259052363}, {\"sample_means\": 4.9678526974529245, \"quantiles\": 4.974276742879644}, {\"sample_means\": 4.968258102176114, \"quantiles\": 4.9760535767700445}, {\"sample_means\": 4.970691808765473, \"quantiles\": 4.977829806044997}, {\"sample_means\": 4.970973993602828, \"quantiles\": 4.979605475918649}, {\"sample_means\": 4.971409282758698, \"quantiles\": 4.981380631506002}, {\"sample_means\": 4.971537331987054, \"quantiles\": 4.983155317831018}, {\"sample_means\": 4.977534508268116, \"quantiles\": 4.984929579834669}, {\"sample_means\": 4.978676661561477, \"quantiles\": 4.986703462382955}, {\"sample_means\": 4.979064588975865, \"quantiles\": 4.988477010274893}, {\"sample_means\": 4.982429536378692, \"quantiles\": 4.990250268250462}, {\"sample_means\": 4.982657650660655, \"quantiles\": 4.992023280998534}, {\"sample_means\": 4.985752478621282, \"quantiles\": 4.993796093164774}, {\"sample_means\": 4.986586262968982, \"quantiles\": 4.995568749359525}, {\"sample_means\": 4.986705715927238, \"quantiles\": 4.99734129416567}, {\"sample_means\": 4.986756339321309, \"quantiles\": 4.99911377214649}, {\"sample_means\": 4.987759438481251, \"quantiles\": 5.00088622785351}, {\"sample_means\": 4.988772173589796, \"quantiles\": 5.00265870583433}, {\"sample_means\": 4.989773067703028, \"quantiles\": 5.004431250640475}, {\"sample_means\": 4.990557042982149, \"quantiles\": 5.006203906835226}, {\"sample_means\": 4.993306355979337, \"quantiles\": 5.007976719001466}, {\"sample_means\": 4.993654933292682, \"quantiles\": 5.009749731749538}, {\"sample_means\": 4.993960060889396, \"quantiles\": 5.011522989725107}, {\"sample_means\": 4.9982922748712175, \"quantiles\": 5.013296537617045}, {\"sample_means\": 4.998502342538598, \"quantiles\": 5.015070420165331}, {\"sample_means\": 4.99952131028116, \"quantiles\": 5.016844682168982}, {\"sample_means\": 4.9997925118131805, \"quantiles\": 5.018619368493998}, {\"sample_means\": 5.003773878079006, \"quantiles\": 5.020394524081351}, {\"sample_means\": 5.004084662860232, \"quantiles\": 5.022170193955003}, {\"sample_means\": 5.005256957617553, \"quantiles\": 5.0239464232299555}, {\"sample_means\": 5.008654835574828, \"quantiles\": 5.025723257120356}, {\"sample_means\": 5.011563127793258, \"quantiles\": 5.027500740947637}, {\"sample_means\": 5.013077450587215, \"quantiles\": 5.029278920148707}, {\"sample_means\": 5.013857635761817, \"quantiles\": 5.031057840284202}, {\"sample_means\": 5.0143097101606715, \"quantiles\": 5.032837547046785}, {\"sample_means\": 5.015743427471612, \"quantiles\": 5.034618086269515}, {\"sample_means\": 5.016531833808615, \"quantiles\": 5.036399503934273}, {\"sample_means\": 5.018270981580155, \"quantiles\": 5.038181846180267}, {\"sample_means\": 5.018735649956893, \"quantiles\": 5.039965159312602}, {\"sample_means\": 5.019233634775677, \"quantiles\": 5.041749489810932}, {\"sample_means\": 5.0199861734684585, \"quantiles\": 5.043534884338194}, {\"sample_means\": 5.023755460647315, \"quantiles\": 5.045321389749431}, {\"sample_means\": 5.025370175936202, \"quantiles\": 5.047109053100699}, {\"sample_means\": 5.025985657487956, \"quantiles\": 5.048897921658072}, {\"sample_means\": 5.026055654366254, \"quantiles\": 5.0506880429067555}, {\"sample_means\": 5.027614301016538, \"quantiles\": 5.052479464560293}, {\"sample_means\": 5.027803850233538, \"quantiles\": 5.054272234569887}, {\"sample_means\": 5.030846957239588, \"quantiles\": 5.056066401133837}, {\"sample_means\": 5.0336948307123786, \"quantiles\": 5.05786201270709}, {\"sample_means\": 5.035959409393455, \"quantiles\": 5.059659118010926}, {\"sample_means\": 5.036243697238544, \"quantiles\": 5.061457766042765}, {\"sample_means\": 5.038033148494072, \"quantiles\": 5.06325800608611}, {\"sample_means\": 5.0410489745573335, \"quantiles\": 5.065059887720633}, {\"sample_means\": 5.041738487231174, \"quantiles\": 5.06686346083241}, {\"sample_means\": 5.042654219739315, \"quantiles\": 5.068668775624301}, {\"sample_means\": 5.04290833178414, \"quantiles\": 5.070475882626495}, {\"sample_means\": 5.0446695881981, \"quantiles\": 5.072284832707212}, {\"sample_means\": 5.045167540598929, \"quantiles\": 5.074095677083586}, {\"sample_means\": 5.047295122297279, \"quantiles\": 5.0759084673327095}, {\"sample_means\": 5.0477273665677505, \"quantiles\": 5.0777232554028755}, {\"sample_means\": 5.049681488086493, \"quantiles\": 5.0795400936250035}, {\"sample_means\": 5.052260351980116, \"quantiles\": 5.081359034724261}, {\"sample_means\": 5.052508748436088, \"quantiles\": 5.083180131831889}, {\"sample_means\": 5.056441808778636, \"quantiles\": 5.085003438497247}, {\"sample_means\": 5.059683716895002, \"quantiles\": 5.086829008700063}, {\"sample_means\": 5.059866517099286, \"quantiles\": 5.088656896862923}, {\"sample_means\": 5.060305317131627, \"quantiles\": 5.090487157863988}, {\"sample_means\": 5.06210284593408, \"quantiles\": 5.092319847049957}, {\"sample_means\": 5.073762716343343, \"quantiles\": 5.094155020249282}, {\"sample_means\": 5.075674763930585, \"quantiles\": 5.095992733785638}, {\"sample_means\": 5.0772760914324735, \"quantiles\": 5.097833044491672}, {\"sample_means\": 5.077674375817654, \"quantiles\": 5.099676009723022}, {\"sample_means\": 5.079128629450803, \"quantiles\": 5.101521687372628}, {\"sample_means\": 5.082082420637927, \"quantiles\": 5.103370135885343}, {\"sample_means\": 5.0845063414503215, \"quantiles\": 5.105221414272848}, {\"sample_means\": 5.085514090755195, \"quantiles\": 5.107075582128893}, {\"sample_means\": 5.092021793145692, \"quantiles\": 5.108932699644861}, {\"sample_means\": 5.093006353327244, \"quantiles\": 5.110792827625681}, {\"sample_means\": 5.096890825322571, \"quantiles\": 5.112656027506089}, {\"sample_means\": 5.098100547853806, \"quantiles\": 5.114522361367256}, {\"sample_means\": 5.10124137677968, \"quantiles\": 5.116391891953798}, {\"sample_means\": 5.103018047805525, \"quantiles\": 5.118264682691172}, {\"sample_means\": 5.107361635741113, \"quantiles\": 5.120140797703479}, {\"sample_means\": 5.107707330606152, \"quantiles\": 5.122020301831691}, {\"sample_means\": 5.110038921849873, \"quantiles\": 5.123903260652299}, {\"sample_means\": 5.110337696028633, \"quantiles\": 5.125789740496426}, {\"sample_means\": 5.110867287899243, \"quantiles\": 5.127679808469387}, {\"sample_means\": 5.111272176012115, \"quantiles\": 5.129573532470744}, {\"sample_means\": 5.111814502767986, \"quantiles\": 5.131470981214848}, {\"sample_means\": 5.112436406379206, \"quantiles\": 5.133372224251902}, {\"sample_means\": 5.1140172616032205, \"quantiles\": 5.135277331989548}, {\"sample_means\": 5.114514368852573, \"quantiles\": 5.137186375715017}, {\"sample_means\": 5.123678366795697, \"quantiles\": 5.13909942761784}, {\"sample_means\": 5.125754481552179, \"quantiles\": 5.141016560813151}, {\"sample_means\": 5.127106463562565, \"quantiles\": 5.142937849365611}, {\"sample_means\": 5.1283713869025975, \"quantiles\": 5.144863368313953}, {\"sample_means\": 5.129362615311724, \"quantiles\": 5.14679319369619}, {\"sample_means\": 5.131201485772481, \"quantiles\": 5.148727402575509}, {\"sample_means\": 5.131217728857029, \"quantiles\": 5.150666073066853}, {\"sample_means\": 5.133559937331827, \"quantiles\": 5.152609284364252}, {\"sample_means\": 5.134847736957581, \"quantiles\": 5.154557116768901}, {\"sample_means\": 5.135761740932512, \"quantiles\": 5.156509651718018}, {\"sample_means\": 5.137624492880713, \"quantiles\": 5.158466971814531}, {\"sample_means\": 5.138290523829569, \"quantiles\": 5.160429160857591}, {\"sample_means\": 5.142183361154235, \"quantiles\": 5.162396303873971}, {\"sample_means\": 5.142425977242178, \"quantiles\": 5.164368487150368}, {\"sample_means\": 5.143462958579476, \"quantiles\": 5.166345798266647}, {\"sample_means\": 5.144870379750695, \"quantiles\": 5.168328326130064}, {\"sample_means\": 5.1490165724233155, \"quantiles\": 5.170316161010497}, {\"sample_means\": 5.150205643240472, \"quantiles\": 5.172309394576739}, {\"sample_means\": 5.1503378473254156, \"quantiles\": 5.174308119933868}, {\"sample_means\": 5.153493119958773, \"quantiles\": 5.176312431661775}, {\"sample_means\": 5.153877994865584, \"quantiles\": 5.1783224258548435}, {\"sample_means\": 5.154945727542729, \"quantiles\": 5.180338200162879}, {\"sample_means\": 5.156573521179968, \"quantiles\": 5.18235985383329}, {\"sample_means\": 5.160233418095215, \"quantiles\": 5.184387487754606}, {\"sample_means\": 5.163128329407061, \"quantiles\": 5.186421204501362}, {\"sample_means\": 5.16333906913139, \"quantiles\": 5.1884611083804195}, {\"sample_means\": 5.165032827830157, \"quantiles\": 5.190507305478766}, {\"sample_means\": 5.174273627457842, \"quantiles\": 5.192559903712871}, {\"sample_means\": 5.1761944144033984, \"quantiles\": 5.1946190128796434}, {\"sample_means\": 5.177006846503148, \"quantiles\": 5.196684744709072}, {\"sample_means\": 5.179357587757811, \"quantiles\": 5.1987572129186015}, {\"sample_means\": 5.181023745234411, \"quantiles\": 5.200836533269337}, {\"sample_means\": 5.182739574743718, \"quantiles\": 5.202922823624129}, {\"sample_means\": 5.183659252640402, \"quantiles\": 5.205016204007639}, {\"sample_means\": 5.183880568791438, \"quantiles\": 5.207116796668455}, {\"sample_means\": 5.18703780347711, \"quantiles\": 5.209224726143351}, {\"sample_means\": 5.188207658348765, \"quantiles\": 5.211340119323781}, {\"sample_means\": 5.188757082645543, \"quantiles\": 5.213463105524704}, {\"sample_means\": 5.190774121215875, \"quantiles\": 5.2155938165558355}, {\"sample_means\": 5.190916661223648, \"quantiles\": 5.217732386795447}, {\"sample_means\": 5.1914326638448305, \"quantiles\": 5.219878953266803}, {\"sample_means\": 5.193214444905888, \"quantiles\": 5.222033655717371}, {\"sample_means\": 5.195157337331779, \"quantiles\": 5.224196636700928}, {\"sample_means\": 5.200029217631757, \"quantiles\": 5.2263680416626705}, {\"sample_means\": 5.20169444660745, \"quantiles\": 5.228548019027507}, {\"sample_means\": 5.203961702816316, \"quantiles\": 5.230736720291623}, {\"sample_means\": 5.206414083816078, \"quantiles\": 5.232934300117528}, {\"sample_means\": 5.206878351193692, \"quantiles\": 5.235140916432699}, {\"sample_means\": 5.209155534840054, \"quantiles\": 5.237356730532008}, {\"sample_means\": 5.214704533657584, \"quantiles\": 5.239581907184123}, {\"sample_means\": 5.216864716637222, \"quantiles\": 5.241816614742039}, {\"sample_means\": 5.221890424132402, \"quantiles\": 5.244061025257979}, {\"sample_means\": 5.222288658573111, \"quantiles\": 5.246315314602835}, {\"sample_means\": 5.222436330013651, \"quantiles\": 5.248579662590394}, {\"sample_means\": 5.230335085780777, \"quantiles\": 5.250854253106584}, {\"sample_means\": 5.231739235992949, \"quantiles\": 5.253139274243969}, {\"sample_means\": 5.233128363754807, \"quantiles\": 5.255434918441781}, {\"sample_means\": 5.233538176764512, \"quantiles\": 5.257741382631746}, {\"sample_means\": 5.234320192394452, \"quantiles\": 5.260058868390012}, {\"sample_means\": 5.234695376100941, \"quantiles\": 5.262387582095475}, {\"sample_means\": 5.236222815677595, \"quantiles\": 5.264727735094854}, {\"sample_means\": 5.2397027015070465, \"quantiles\": 5.267079543874851}, {\"sample_means\": 5.241105748657842, \"quantiles\": 5.269443230241774}, {\"sample_means\": 5.243543565407252, \"quantiles\": 5.271819021509014}, {\"sample_means\": 5.244058004461275, \"quantiles\": 5.274207150692801}, {\"sample_means\": 5.24724404536965, \"quantiles\": 5.276607856716688}, {\"sample_means\": 5.251156962155989, \"quantiles\": 5.279021384625223}, {\"sample_means\": 5.255102219559282, \"quantiles\": 5.281447985807341}, {\"sample_means\": 5.257753609009842, \"quantiles\": 5.2838879182299765}, {\"sample_means\": 5.26410812991159, \"quantiles\": 5.286341446682518}, {\"sample_means\": 5.26581963599719, \"quantiles\": 5.288808843032668}, {\"sample_means\": 5.269324591255987, \"quantiles\": 5.291290386494398}, {\"sample_means\": 5.26999220312312, \"quantiles\": 5.29378636390867}, {\"sample_means\": 5.275742511006965, \"quantiles\": 5.296297070037684}, {\"sample_means\": 5.283876505925432, \"quantiles\": 5.298822807873438}, {\"sample_means\": 5.285877339461902, \"quantiles\": 5.301363888961454}, {\"sample_means\": 5.287062258017077, \"quantiles\": 5.3039206337405815}, {\"sample_means\": 5.2880314313611985, \"quantiles\": 5.306493371899843}, {\"sample_means\": 5.28974089127284, \"quantiles\": 5.309082442753384}, {\"sample_means\": 5.294167950069214, \"quantiles\": 5.311688195634627}, {\"sample_means\": 5.300835986290361, \"quantiles\": 5.314310990310843}, {\"sample_means\": 5.304375876301386, \"quantiles\": 5.316951197419436}, {\"sample_means\": 5.304794974732209, \"quantiles\": 5.319609198927309}, {\"sample_means\": 5.310129068223595, \"quantiles\": 5.3222853886148265}, {\"sample_means\": 5.314463593870843, \"quantiles\": 5.324980172585957}, {\"sample_means\": 5.321230720612464, \"quantiles\": 5.327693969806352}, {\"sample_means\": 5.321369436673149, \"quantiles\": 5.330427212671193}, {\"sample_means\": 5.321689506406893, \"quantiles\": 5.33318034760486}, {\"sample_means\": 5.327758519241338, \"quantiles\": 5.335953835694559}, {\"sample_means\": 5.329153026675224, \"quantiles\": 5.338748153360282}, {\"sample_means\": 5.33594756638565, \"quantiles\": 5.341563793063633}, {\"sample_means\": 5.337157344864686, \"quantiles\": 5.344401264058278}, {\"sample_means\": 5.338615661740521, \"quantiles\": 5.347261093185001}, {\"sample_means\": 5.342669484795178, \"quantiles\": 5.350143825714599}, {\"sample_means\": 5.343608485902256, \"quantiles\": 5.353050026242153}, {\"sample_means\": 5.348383444307265, \"quantiles\": 5.355980279636469}, {\"sample_means\": 5.350853352050424, \"quantiles\": 5.358935192048877}, {\"sample_means\": 5.351664988204009, \"quantiles\": 5.361915391985907}, {\"sample_means\": 5.357784953210037, \"quantiles\": 5.36492153145078}, {\"sample_means\": 5.364752287345396, \"quantiles\": 5.367954287159134}, {\"sample_means\": 5.367304811111495, \"quantiles\": 5.371014361834856}, {\"sample_means\": 5.368376775391115, \"quantiles\": 5.374102485592492}, {\"sample_means\": 5.371643661460351, \"quantiles\": 5.377219417413311}, {\"sample_means\": 5.388552859872966, \"quantiles\": 5.3803659467227645}, {\"sample_means\": 5.389831068782667, \"quantiles\": 5.38354289507788}, {\"sample_means\": 5.390274966345342, \"quantiles\": 5.386751117973951}, {\"sample_means\": 5.394299431444426, \"quantiles\": 5.389991506780851}, {\"sample_means\": 5.395415934055975, \"quantiles\": 5.39326499082034}, {\"sample_means\": 5.397544937249908, \"quantiles\": 5.3965725395969475}, {\"sample_means\": 5.404154684731577, \"quantiles\": 5.399915165196323}, {\"sample_means\": 5.406546147096911, \"quantiles\": 5.403293924866451}, {\"sample_means\": 5.406923624525368, \"quantiles\": 5.406709923798809}, {\"sample_means\": 5.41269447794835, \"quantiles\": 5.410164318128445}, {\"sample_means\": 5.420199153340281, \"quantiles\": 5.413658318174088}, {\"sample_means\": 5.422678183632788, \"quantiles\": 5.417193191941835}, {\"sample_means\": 5.4228950310135495, \"quantiles\": 5.420770268918708}, {\"sample_means\": 5.426587914269785, \"quantiles\": 5.424390944185481}, {\"sample_means\": 5.437827323483633, \"quantiles\": 5.428056682881742}, {\"sample_means\": 5.440264992304073, \"quantiles\": 5.431769025060229}, {\"sample_means\": 5.452613910883091, \"quantiles\": 5.435529590972059}, {\"sample_means\": 5.463237309996876, \"quantiles\": 5.439340086829846}, {\"sample_means\": 5.466268958831558, \"quantiles\": 5.443202311101771}, {\"sample_means\": 5.473657581277157, \"quantiles\": 5.447118161396703}, {\"sample_means\": 5.4738638356110805, \"quantiles\": 5.451089642008585}, {\"sample_means\": 5.475317301020958, \"quantiles\": 5.455118872197725}, {\"sample_means\": 5.478202204188282, \"quantiles\": 5.459208095297508}, {\"sample_means\": 5.479398735968765, \"quantiles\": 5.463359688747764}, {\"sample_means\": 5.482126741148438, \"quantiles\": 5.467576175170895}, {\"sample_means\": 5.482607536454591, \"quantiles\": 5.471860234624201}, {\"sample_means\": 5.492127307543335, \"quantiles\": 5.4762147181823355}, {\"sample_means\": 5.4967296809039565, \"quantiles\": 5.480642663027866}, {\"sample_means\": 5.50074581106533, \"quantiles\": 5.485147309256504}, {\"sample_means\": 5.501039365593764, \"quantiles\": 5.489732118637398}, {\"sample_means\": 5.5024194070053385, \"quantiles\": 5.4944007956093195}, {\"sample_means\": 5.5092621228062395, \"quantiles\": 5.499157310841976}, {\"sample_means\": 5.519390057215971, \"quantiles\": 5.504005927749817}, {\"sample_means\": 5.525098491788846, \"quantiles\": 5.508951232416014}, {\"sample_means\": 5.525104469891029, \"quantiles\": 5.513998167469502}, {\"sample_means\": 5.52646950882653, \"quantiles\": 5.519152070561983}, {\"sample_means\": 5.549620198183098, \"quantiles\": 5.524418718219132}, {\"sample_means\": 5.551611215927092, \"quantiles\": 5.529804375997294}, {\"sample_means\": 5.554511336150199, \"quantiles\": 5.535315856071474}, {\"sample_means\": 5.555677064360494, \"quantiles\": 5.5409605836230025}, {\"sample_means\": 5.56526171441417, \"quantiles\": 5.546746673699616}, {\"sample_means\": 5.596921569615228, \"quantiles\": 5.552683020605155}, {\"sample_means\": 5.599539433816392, \"quantiles\": 5.558779402365258}, {\"sample_means\": 5.6001238341604225, \"quantiles\": 5.565046603442623}, {\"sample_means\": 5.605606060449961, \"quantiles\": 5.571496559686004}, {\"sample_means\": 5.614286205153738, \"quantiles\": 5.578142530554111}, {\"sample_means\": 5.617100001409678, \"quantiles\": 5.58499930504646}, {\"sample_means\": 5.636838030498286, \"quantiles\": 5.592083449622059}, {\"sample_means\": 5.646308596927911, \"quantiles\": 5.599413608870787}, {\"sample_means\": 5.648262340798547, \"quantiles\": 5.6070108730784805}, {\"sample_means\": 5.654610515155002, \"quantiles\": 5.6148992314735064}, {\"sample_means\": 5.663850772634603, \"quantiles\": 5.623106136425749}, {\"sample_means\": 5.6722513815297155, \"quantiles\": 5.6316632130494755}, {\"sample_means\": 5.684510076114676, \"quantiles\": 5.640607161874531}, {\"sample_means\": 5.687325226941516, \"quantiles\": 5.649980921609479}, {\"sample_means\": 5.7082905456451245, \"quantiles\": 5.659835187949408}, {\"sample_means\": 5.731973304166123, \"quantiles\": 5.670230428567944}, {\"sample_means\": 5.7498006096143, \"quantiles\": 5.68123960359658}, {\"sample_means\": 5.762753762142653, \"quantiles\": 5.692951912174839}, {\"sample_means\": 5.841508837145807, \"quantiles\": 5.70547807037697}, {\"sample_means\": 5.841692875927888, \"quantiles\": 5.718957943608157}, {\"sample_means\": 5.844410973983822, \"quantiles\": 5.733571926333261}, {\"sample_means\": 5.869041415039901, \"quantiles\": 5.749558534209148}, {\"sample_means\": 5.897021314476323, \"quantiles\": 5.767242810888859}, {\"sample_means\": 5.912292075034993, \"quantiles\": 5.787084719220738}, {\"sample_means\": 5.919866096135929, \"quantiles\": 5.809767328921651}, {\"sample_means\": 5.932643302721357, \"quantiles\": 5.83637230960178}, {\"sample_means\": 5.9703289070234495, \"quantiles\": 5.868773803187855}, {\"sample_means\": 6.000975590939692, \"quantiles\": 5.910693183859225}, {\"sample_means\": 6.1764989487759525, \"quantiles\": 5.9714874254331605}, {\"sample_means\": 6.3860885893518535, \"quantiles\": 6.092562109566502}], \"data-48b6d2e21e0069851a24f831171f2869\": [{\"x\": 3.9415746891888053, \"y\": 3.9415746891888053}, {\"x\": 6.486088589351853, \"y\": 6.486088589351853}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_mesh = [1 - (1 / len(sample_means)) * ix - (0.5 / len(sample_means)) for ix in range(0,len(sample_means))]\n",
    "quantile_vals = [stats.norm.isf(u, loc = 5, scale = clt_scale) for u in unit_mesh]\n",
    "sample_means.sort()\n",
    "quant_df = pd.DataFrame({\n",
    "    \"sample_means\": sample_means,\n",
    "    \"quantiles\": quantile_vals})\n",
    "\n",
    "ab_lims = [min(sample_means)-0.1, max(sample_means)+0.1]\n",
    "abline_df = pd.DataFrame({\"x\": ab_lims, \"y\": ab_lims})\n",
    "\n",
    "quant_points = (alt\n",
    "                   .Chart(quant_df)\n",
    "                   .mark_point()\n",
    "                   .encode(x =  alt.X('sample_means:Q',\n",
    "                                      scale=alt.Scale(zero=False),\n",
    "                                      title = \"Sample mean quantile\"),\n",
    "                           y = alt.Y('quantiles:Q',\n",
    "                                      scale=alt.Scale(zero=False),\n",
    "                                      title = \"Normal quantile\")))\n",
    "\n",
    "ab_line = (alt\n",
    "          .Chart(abline_df)\n",
    "          .mark_line(color = \"red\")\n",
    "          .encode(x = \"x\", y = \"y\"))\n",
    "\n",
    "quant_points + ab_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental results: flipping coins in series\n",
    "\n",
    "Professor Mittens asked 15 of his students to each take turns flipping a coin 30 times and recording how many heads they got. He has a sneaking suspicion that some of the students did not actually do this properly, that they just wrote down some garbage and went to lunch early. We will help Mittens work out whether the coin that was used was fair, i.e. has an equal chance of showing heads or tails.\n",
    "\n",
    "### Exercise 3 part I\n",
    "\n",
    "Read the data in `experiement1.csv` into a `DataFrame`. Use some of the commands you've seen in lectures to inspect the data, noting that it may not be in the optimal format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>flip_number</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.325302</td>\n",
       "      <td>8.665075</td>\n",
       "      <td>0.500512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  flip_number     outcome\n",
       "count  450.000000   450.000000  450.000000\n",
       "mean     7.000000    15.500000    0.493333\n",
       "std      4.325302     8.665075    0.500512\n",
       "min      0.000000     1.000000    0.000000\n",
       "25%      3.000000     8.000000    0.000000\n",
       "50%      7.000000    15.500000    0.000000\n",
       "75%     11.000000    23.000000    1.000000\n",
       "max     14.000000    30.000000    1.000000"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"experiment1.csv\")\n",
    "df1.describe()\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 part II\n",
    "\n",
    "Compute the point estimate and CI using the function you wrote above. Write a sentence explaining whether you think the coin is a _fair_ coin given the results you have just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49333333333333335\n",
      "(0.44713979693549655, 0.5395268697311701)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "wald_estimate_and_ci(num_trials = len(df1), num_success = df1['outcome'].sum())\n",
    "\n",
    "p = wald_estimate_and_ci(num_trials = len(df1), num_success = df1['outcome'].sum())[0]\n",
    "ci = wald_estimate_and_ci(num_trials = len(df1), num_success = df1['outcome'].sum())[1]\n",
    "\n",
    "print(p)\n",
    "print(ci)\n",
    "print(ci[0] < 0.5 < ci[1]) \n",
    "# Coin is a fair coin - we estimate the probability of heads as 0.49 with a CI of [0.45, 0.54], i.e. cannot reject the null hypothesis that p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 part III\n",
    "\n",
    "Generate a histogram of the number of heads from each student. As an extension, include the binomial distribution supported by your estimate that is most amenable to large value outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['flip_number'], axis = 1).groupby(\"name\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'No. of Successes')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mervyncheong/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2191\u001b[0m                            else suppress())\n\u001b[1;32m   2192\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m                     bbox_inches = self.figure.get_tightbbox(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_draw_path_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdraw_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mtpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_path_non_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0maffine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_affine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m\"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \"\"\"\n\u001b[1;32m    770\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mrot_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mfrom_extents\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maxis\u001b[0m \u001b[0mincreases\u001b[0m \u001b[0mupwards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \"\"\"\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \"\"\"\n\u001b[1;32m    773\u001b[0m         \u001b[0mBboxBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise ValueError('Bbox points must be of the form '\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(x = , y = df1['outcome'])\n",
    "plt.xlabel(\"No. of Successes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 part I\n",
    "\n",
    "It looks like there might be a couple of strange points in this dataset as Mittens suspected. Using the upper bound on $p$ calculate the probability of someone getting all heads. Write a couple of sentences explaining whether you think it is reasonable to remove those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.128351480161736e-09"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.binom.pmf(k = 30, n = 30, p = ci[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 part II\n",
    "\n",
    "Remove the outliers and repeat the process of plotting the data and estimating the parameters and CI. Once you have done this, plot the distribution of the estimated binomial distribution on top of the histogram. Write a couple of sentences explaining what you think about the coin now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4153846153846154, (0.3664761437453554, 0.4642930870238754))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_cleaned = df1[df1['outcome'] < 30]\n",
    "wald_estimate_and_ci(num_success = df1_cleaned['outcome'].sum(), num_trials = df1_cleaned.size * 30)\n",
    "\n",
    "### After removing two outliers, we estimate the probability of heads as $0.42$ with a $95\\%$ CI of $(0.37, 0.46)$.\n",
    "### Given the CI does not contain the value 0.5, we can reject the null hypothesis that the coin is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental results: flipping coins in parallel\n",
    "\n",
    "After the success of his first experiment, Mittens was lauded as a statistical wizard. The royal mint has become interested and is providing additional funds to obtain an additional 49 coins and repeat the experiment to gather more data about the fascinating topic of coin bias. Now he gives each of 50 students a coin each and asks them to flip the coin 30 times and record the results. We will help Mittens work out whether the coins are fair.\n",
    "\n",
    "### Excercise 5 part I\n",
    "\n",
    "Do we need to change anything about how we analyse this data? If so, why, if not, why not? **Hint:** there are good arguments that can be given for each answer. Once you have answered one way, try to answer the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not need to change anything if we assume that the coin is fair, i.e. probability of heads = 0.5\n",
    "# If you assume that the coins vary then we need to adjust the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 part II\n",
    "\n",
    "Using the data in `experiment2.csv` explore the data set using the methodology devised above and write a couple of sentences to explain what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4013333333333333\n",
      "(0.37652739859779666, 0.42613926806887)\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  1.,  5.,  4.,  8.,  9.,  1., 11.,  4.,  3.]),\n",
       " array([ 3. ,  4.7,  6.4,  8.1,  9.8, 11.5, 13.2, 14.9, 16.6, 18.3, 20. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"experiment2.csv\")\n",
    "df2 = df2.drop(['flip_number'], axis = 1).groupby(\"name\").sum()\n",
    "\n",
    "wald_estimate_and_ci(num_trials = len(df2) * 30, num_success = df2['outcome'].sum())\n",
    "\n",
    "p = wald_estimate_and_ci(num_trials = len(df2) * 30, num_success = df2['outcome'].sum())[0]\n",
    "ci = wald_estimate_and_ci(num_trials = len(df2) * 30, num_success = df2['outcome'].sum())[1]\n",
    "\n",
    "print(p)\n",
    "print(ci)\n",
    "print(ci[0] < 0.5 < ci[1]) \n",
    "plt.hist(df2['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 part III\n",
    "\n",
    "Visualise the number of heads each student got and compare the variance in this to what is predicted by theory. Revise your answer to part I of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.692244897959185 7.2079466666666665\n"
     ]
    }
   ],
   "source": [
    "actual_var = df2['outcome'].var()\n",
    "theoretical_var = stats.binom.var(n = 30, p = p)\n",
    "\n",
    "print(actual_var, theoretical_var)\n",
    "\n",
    "## there appears to be systematic differences in the number of heads obtained\n",
    "## the variance is too great\n",
    "## seems like we should not consider all coins as equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 part IV (Extension)\n",
    "\n",
    "Consider how you might analyse this data. Over the following weeks you will learn a couple of approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one could use a negative binomial distribution / beta binomial distribution if one were interested in an empirical analysis \n",
    "## or a hierarchical model if one were interested in a structural analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue\n",
    "\n",
    "Professor Mittens' work was published in a top tier journal and he was lauded as a statistical wizard. Rumour has it he will soon be elected to the British Acadmey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
